{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP (Natural Language Processing) com Python\n",
    "\n",
    "Este é o notebook que acompanha o de vídeo de NLP!\n",
    "\n",
    "Nesta palestra discutiremos uma visão geral de processamento de linguagem natural, que basicamente consiste em combinar técnicas de Machine Learning com texto e usando matemática e estatísticas para obter esse texto em um formato que os algoritmos de aprendizado de máquina possam entender!\n",
    "\n",
    "Depois de concluir esta palestra, você terá um projeto usando alguns dados de texto da Yelp!\n",
    "    \n",
    "** Requisitos: você precisará ter NLTK instalado, além de baixar o corpus para palavras-passe. Para baixar tudo com uma instalação conda, execute a célula abaixo. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.12\n",
      "  latest version: 4.8.3\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/felipe/Python\n",
      "\n",
      "  added / updated specs: \n",
      "    - nltk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    pip-20.0.2                 |           py38_1         1.9 MB\n",
      "    sqlite-3.31.1              |       h62c20be_1         2.0 MB\n",
      "    xz-5.2.5                   |       h7b6447c_0         438 KB\n",
      "    readline-8.0               |       h7b6447c_0         428 KB\n",
      "    openssl-1.1.1g             |       h7b6447c_0         3.8 MB\n",
      "    certifi-2020.4.5.1         |           py38_0         159 KB\n",
      "    wheel-0.34.2               |           py38_0          49 KB\n",
      "    nltk-3.4.5                 |           py38_0         2.1 MB\n",
      "    setuptools-46.1.3          |           py38_0         656 KB\n",
      "    ncurses-6.2                |       he6710b0_1         1.1 MB\n",
      "    ld_impl_linux-64-2.33.1    |       h53a641e_7         645 KB\n",
      "    libedit-3.1.20181209       |       hc058e9b_0         188 KB\n",
      "    libgcc-ng-9.1.0            |       hdf63c60_0         8.1 MB\n",
      "    ca-certificates-2020.1.1   |                0         132 KB\n",
      "    python-3.8.2               |       hcf32534_0        57.8 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        79.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    _libgcc_mutex:    0.1-main               \n",
      "    ld_impl_linux-64: 2.33.1-h53a641e_7      \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates:  2018.03.07-0            --> 2020.1.1-0             \n",
      "    certifi:          2018.11.29-py37_0       --> 2020.4.5.1-py38_0      \n",
      "    libedit:          3.1.20170329-h6b74fdf_2 --> 3.1.20181209-hc058e9b_0\n",
      "    libgcc-ng:        8.2.0-hdf63c60_1        --> 9.1.0-hdf63c60_0       \n",
      "    ncurses:          6.1-he6710b0_1          --> 6.2-he6710b0_1         \n",
      "    nltk:             3.4-py37_1              --> 3.4.5-py38_0           \n",
      "    openssl:          1.1.1a-h7b6447c_0       --> 1.1.1g-h7b6447c_0      \n",
      "    pip:              18.1-py37_0             --> 20.0.2-py38_1          \n",
      "    python:           3.7.1-h0371630_7        --> 3.8.2-hcf32534_0       \n",
      "    readline:         7.0-h7b6447c_5          --> 8.0-h7b6447c_0         \n",
      "    setuptools:       40.6.3-py37_0           --> 46.1.3-py38_0          \n",
      "    six:              1.12.0-py37_0           --> 1.14.0-py38_0          \n",
      "    sqlite:           3.26.0-h7b6447c_0       --> 3.31.1-h62c20be_1      \n",
      "    wheel:            0.32.3-py37_0           --> 0.34.2-py38_0          \n",
      "    xz:               5.2.4-h14c3975_4        --> 5.2.5-h7b6447c_0       \n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!conda install nltk\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INÍCIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos olhar outras sub-bibliotecas da famosa biblioteca nltk e baixar a única que precisaremos \n",
    "# para este projeto específico, que é a sub-biblioteca \"stopwords\".\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtendo os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos um conjunto de dados da [UCI](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)! Este conjunto de dados já está localizado na pasta para esta seção."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso dever é escrever um modelo de NLP que opere numa caixa de arquivos de email e preveja se as mensagens são de spam ou ham.\n",
    "\n",
    "\n",
    "O arquivo que estamos usando contém uma coleção de mais de 5 mil mensagens SMS. Você pode checar o arquivo ** readme ** para obter mais informações.\n",
    "\n",
    "Vamos continuar usando Rstrip() e uma lista de compreensão para obter uma lista de todas as linhas de mensagens de texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desta vez vamos utilizar uma forma diferente de ler o nosso conjunto de dados. Para cada linha no método abrir colocaremos uma string que contém o caminho para abrir o nosso arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [line.rstrip() for line in open('/media/felipe/SAMSUNG/LAPTOP RECENTE/felipe/Python3.7/udemyCOURSES/PythonParaDataScienceEMachineLearning/Python-Data-Science-and-Machine-Learning-Bootcamp/5. Machine Learning/Processamento de linguagem natural/smsspamcollection/SMSSpamCollection')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "# Temos uma lista com 5574 mensagens\n",
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vejamos algumas das mensagens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham\\tOk lar... Joking wif u oni...'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam\\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uma coleção de textos também é chamado às vezes de \"corpus\". Vamos imprimir as 15 primeiras mensagens e numerá-las usando ** enumerate **:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "\n",
      "1 ham\tOk lar... Joking wif u oni...\n",
      "\n",
      "\n",
      "2 spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "\n",
      "3 ham\tU dun say so early hor... U c already then say...\n",
      "\n",
      "\n",
      "4 ham\tNah I don't think he goes to usf, he lives around here though\n",
      "\n",
      "\n",
      "5 spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
      "\n",
      "\n",
      "6 ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "\n",
      "\n",
      "7 ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "\n",
      "\n",
      "8 spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "\n",
      "\n",
      "9 spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "\n",
      "\n",
      "10 ham\tI'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
      "\n",
      "\n",
      "11 spam\tSIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\n",
      "\n",
      "\n",
      "12 spam\tURGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\n",
      "\n",
      "\n",
      "13 ham\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\n",
      "\n",
      "\n",
      "14 ham\tI HAVE A DATE ON SUNDAY WITH WILL!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message_number, message in enumerate(messages[:15]):\n",
    "    print(message_number, message)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido ao espaçamento, podemos dizer que este é um arquivo [TSV](http://en.wikipedia.org/wiki/Tab-separated_values) (\"valores separados por tabulação\"), onde a primeira coluna é uma etiqueta dizendo se a a mensagem dada é uma mensagem normal (comumente conhecida como \"ham\") ou \"spam\". A segunda coluna é a própria mensagem. (Notemos que nossos números não fazem parte do arquivo, eles são apenas do ** enumerate **).\n",
    "\n",
    "Usando estes exemplos de ham e spam rotulados, iremos treinar um modelo de aprendizado de máquina para aprender a discriminar entre ham / spam automaticamente **. Então, com um modelo treinado, poderemos ** classificar mensagens arbitrárias sem letras ** como ham ou spam.\n",
    "\n",
    "A partir da documentação oficial SciKit Learn, podemos visualizar nosso processo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://www.astroml.org/sklearn_tutorial/_images/plot_ML_flow_chart_3.png' width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Em vez de analisar TSV manualmente usando o Python, podemos aproveitar o pandas para trabalhar melhor com os nossos dados! Vamos continuar e importá-lo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use **read_csv** and make note of the **sep** argument, we can also specify the desired column names by passing in a list of *names*.\n",
    "\n",
    "Usaremos ** read_csv ** e ajustaremos o argumento** sep **. Também podemos especificar os nomes das colunas desejadas passando em uma lista de * nomes *."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Messages = pd.read_csv('/media/felipe/SAMSUNG/LAPTOP RECENTE/felipe/Python3.7/udemyCOURSES/PythonParaDataScienceEMachineLearning/Python-Data-Science-and-Machine-Learning-Bootcamp/5. Machine Learning/Processamento de linguagem natural/smsspamcollection/SMSSpamCollection',\n",
    "                       sep='\\t', names=[\"label\", \"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Messages.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Messages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Messages.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos ver quantas mensagens temos dos tipos ham e spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de dados exploratória\n",
    "\n",
    "Vamos verificar algumas das estatísticas com plots e os métodos incorporados ao pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao continuar nossa análise, queremos começar a pensar nos parâmetrs que usaremos. Isso acompanha a idéia geral de [engenharia de parâmetros](https://en.wikipedia.org/wiki/Feature_engineering). A engenharia de recursos é uma parte muito grande da detecção de spam em geral. Encorajo você a ler sobre o assunto!\n",
    "\n",
    "Vamos criar uma nova coluna para detectar o tamanho das mensagens de texto para que possamos como as mensagens se distribuem nos seus tamanhos. Para isso, usaremos o método len():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Messages['Length'] = Messages['message'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  Length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...     147\n",
       "6   ham  Even my brother is not like to speak with me. ...      77\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...     160\n",
       "8  spam  WINNER!! As a valued network customer you have...     157\n",
       "9  spam  Had your mobile 11 months or more? U R entitle...     154"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Messages.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos fazer um histograma do tamanho das mensagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f593f235668>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "Messages['Length'].plot(kind='hist', bins=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notemos que, a princípio, temos 2 distribuições: uma em torno de 50 e a outra em torno de 150. Temos também uma mensagem com tamanho maior que 800. Vamos visualizá-la:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.489950\n",
       "std        59.942907\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: Length, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Messages.Length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1085    For me the love should start with attraction.i...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Messages[Messages['Length'] == 910]['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para visualizá-la melhor vamos usar o iloc[0]\n",
    "Messages[Messages['Length'] == 910]['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parece que temos algum tipo de texto enviado por um Romeu! Mas vamos nos concentrar na idéia de tentar ver se o comprimento da mensagem é uma característica distintiva entre ham e spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x7f593eba6780>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x7f593eb55da0>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Vamos agora splitar o histograma em 2 plots, um de ham e o outro de spam\n",
    "Messages.hist(bins=100, column='Length', by='label', figsize=(18, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muito interessante! Com apenas EDA básica, conseguimos descobrir uma tendência de que as mensagens de spam tendem a ter mais caracteres (desculpe, Romeu!), ao passo que as mensagens de ham (que são as do bem) tendem a ser menores.\n",
    "\n",
    "Agora vamos começar a processar os dados para que possamos eventualmente usá-lo com o SciKit Learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré- processamento de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso principal problema com nossos dados é que está tudo em formato de texto (strings). Os algoritmos de classificação que aprendemos até agora precisam de algum tipo de vetor de características numéricas para realizar a tarefa de classificação. Na verdade, existem muitos métodos para converter uns textos em um formato vetorial. O mais simples é a abordagem [bag-of-words](http://en.wikipedia.org/wiki/Bag-of-words_model), onde cada palavra única em um texto será representada por um número.\n",
    "\n",
    "\n",
    "Nesta seção, converteremos as mensagens brutas (seqüência de caracteres) em vetores (seqüências de números).\n",
    "\n",
    "Como primeiro passo, vamos escrever uma função que dividirá uma mensagem em suas palavras individuais e retornará uma lista. Também removeremos palavras muito comuns, ('' '', 'a', etc.). Para fazer isso, iremos aproveitar a biblioteca NLTK. É praticamente a biblioteca padrão em Python para processar texto e tem muitos recursos úteis. Nós só usaremos alguns dos básicos aqui.\n",
    "\n",
    "Vamos criar uma função que processará a string na coluna da mensagem, então podemos usar ** apply() ** do pandas para processar todo o texto no DataFrame.\n",
    "\n",
    "Primeiramente eliminamos a pontuação. Podemos aproveitar a biblioteca incorporada ** string ** do Python para obter uma lista rápida de todas as possíveis pontuações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A biblioteca string contém uma lista com todas as pontuações possíveis\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos primeiro aplicar essa técnica de retirar as pontuações nas mensagens exemplares abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess1 = 'Sample message! Notice: it has punctuation.'\n",
    "mess2 = 'Hello man! I would like to tell you something: you are # 10 and dozen = 12.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess1withoutpunctuation = [char for char in mess1 if char not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'a',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'm',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'a',\n",
       " 'g',\n",
       " 'e',\n",
       " ' ',\n",
       " 'N',\n",
       " 'o',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'i',\n",
       " 't',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'p',\n",
       " 'u',\n",
       " 'n',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mess1withoutpunctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample message Notice it has punctuation'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agora vamos rejuntar a mensagem acima usando o método join()\n",
    "mess1withoutpunctuation = ''.join(mess1withoutpunctuation)\n",
    "mess1withoutpunctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello man I would like to tell you something you are  10 and dozen  12'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mess2withoutpunctuation = [char for char in mess2 if char not in string.punctuation]\n",
    "mess2withoutpunctuation = ''.join(mess2withoutpunctuation)\n",
    "mess2withoutpunctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outro exemplo para usar o método join():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C', 'D']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemplo = 'A B C D'.split()\n",
    "exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABCD'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A123B123C123D'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'123'.join(exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A---B---C---D'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'---'.join(exemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos ver como remover stopwords. Podemos importar uma lista de stopwords inglesas da NLTK (verifique a documentação para mais idiomas e informações)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# o parâmetro é o idioma\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos agora escrever uma função que retira as stopwords das mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message!', 'Notice:', 'punctuation.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_mess1 = [word for word in mess1.split() if word.lower() not in stopwords.words('english')]\n",
    "cleaned_mess1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'man!',\n",
       " 'would',\n",
       " 'like',\n",
       " 'tell',\n",
       " 'something:',\n",
       " '#',\n",
       " '10',\n",
       " 'dozen',\n",
       " '=',\n",
       " '12.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_mess2 = [word for word in mess2.split() if word.lower() not in stopwords.words('english')]\n",
    "cleaned_mess2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'man', 'would', 'like', 'tell', 'something', '10', 'dozen', '12']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_mess2withoutpunctuation = [word for word in mess2withoutpunctuation.split() if word.lower() not in stopwords.words('english')]\n",
    "cleaned_mess2withoutpunctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, coloquemos esses dois juntos em uma função para aplicá-lo ao nosso DataFrame mais tarde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    # Retira pontuações\n",
    "    no_punctuation = [char for char in mess if char not in string.punctuation]\n",
    "    \n",
    "    # Junta-os para formar strings\n",
    "    no_punctuation = ''.join(no_punctuation)\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    sms = [word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]\n",
    "    return sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui está o DataFrame original novamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  Length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos \"tokenizar\" essas mensagens. Tokenização é apenas o termo usado para descrever o processo de conversão das cadeias de texto normais em uma lista de tokens (palavras que realmente queremos).\n",
    "\n",
    "Vamos ver um exemplo de saída na coluna:\n",
    "\n",
    "**Nota:**\n",
    "Podemos obter alguns avisos ou erros para símbolos que não contamos ou que não estavam no Unicode (como um símbolo de libra britânica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos verificar nos 5 primeiros elementos do nosso DataFrame para ver se está funcionando\n",
    "Messages['message'].head(5).apply(text_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuação da normalização\n",
    "\n",
    "Há muitas maneiras de continuar normalizando esse texto. Como [Stemming] (https://en.wikipedia.org/wiki/Stemming) ou distinguindo por [parte do discurso] (http://www.nltk.org/book/ch05.html).\n",
    "\n",
    "O NLTK possui muitas ferramentas integradas e UMA excelente documentação em muitos destes métodos. Às vezes, eles não funcionam bem para mensagens de texto devido ao modo como muitas pessoas tendem a usar abreviaturas ou taquigrafia:\n",
    "    \n",
    "    \n",
    "Alguns métodos de normalização de texto terão problemas com este tipo de taquigrafia e, portanto, deixarei você para explorar esses métodos mais avançados através do [NLTK book online](http://www.nltk.org/book/).\n",
    "\n",
    "Por enquanto, apenas nos concentraremos EMs converter nossa lista de palavras para um vetor real que o SciKit-Learn pode usar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vetorização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora temos as mensagens como listas de tokens (também conhecido como [lemmas](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)) e agora nós precisamos converter cada uma dessas mensagens em um vetor com o qual os modelos de algoritmo do SciKit Learn podem funcionar.\n",
    "\n",
    "Agora, converteremos cada mensagem, representada como uma lista de tokens (lemmas) acima, em um vetor que os modelos de aprendizagem de máquinas podem entender.\n",
    "\n",
    "Vamos fazer isso em três etapas usando o modelo bag-of-words:\n",
    "\n",
    "1. Contar quantas vezes ocorre uma palavra em cada mensagem (conhecida como freqüência de termo)\n",
    "\n",
    "2. Pesar as contagens, de modo que tokens freqüentes recebem menor peso (freqüência inversa do documento)\n",
    "\n",
    "3. Normalize os vetores para o comprimento da unidade, para abstrair do comprimento do texto original (norma L2)\n",
    "\n",
    "Vamos começar o primeiro passo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada vetor terá tantas dimensões quanto houverem palavras únicas no corpo do SMS. Em primeiro lugar usaremos o ** CountVectorizer ** do SciKit Learn **. Este modelo converterá uma coleção de documentos de texto em uma matriz de contagem de token.\n",
    "\n",
    "Podemos imaginar isso como uma matriz bidimensional. Onde a dimensão 1 é o vocabulário inteiro (1 linha por palavra) e a outra dimensão são os documentos reais, neste caso uma coluna por mensagem de texto.\n",
    "\n",
    "Por exemplo:\n",
    "\n",
    "<table border = “1“>\n",
    "<tr>\n",
    "<th></th> <th>Message 1</th> <th>Message 2</th> <th>...</th> <th>Message N</th> \n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word 1 Count</b></td><td>0</td><td>1</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word 2 Count</b></td><td>0</td><td>0</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>...</b></td> <td>1</td><td>2</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word N Count</b></td> <td>0</td><td>1</td><td>...</td><td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Uma vez que há tantas mensagens, podemos esperar muitos elementos zerados. Por isso, o SciKit Learn emitirá uma [Matriz Esparsa](https://en.wikipedia.org/wiki/Sparse_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bow abrevia \"Bag of words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há muitos argumentos e parâmetros que podem ser passados para o CountVectorizer. Neste caso, vamos especificar o ** analyzer ** para usar nossa própria função previamente definida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bow_transformer = CountVectorizer(analyzer=text_process).fit(Messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11425\n"
     ]
    }
   ],
   "source": [
    "print(len(Bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notemos que temos 11425 palavras únicas no nosso conjunto de mensagens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora pegar umas mensagens de texto e obter suas contagens de bag-of-words como um vetor, colocando em uso o nosso novo Bow_transformer para vermos como elas aparecem na sua representação vetorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor... U c already then say...\n"
     ]
    }
   ],
   "source": [
    "message4 = Messages['message'][3]\n",
    "print(message4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nah I don't think he goes to usf, he lives around here though\n"
     ]
    }
   ],
   "source": [
    "print(Messages['message'][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora vejamos sua representação vetorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4068)\t2\n",
      "  (0, 4629)\t1\n",
      "  (0, 5261)\t1\n",
      "  (0, 6204)\t1\n",
      "  (0, 6222)\t1\n",
      "  (0, 7186)\t1\n",
      "  (0, 9554)\t2\n",
      "\n",
      "\n",
      "(1, 11425)\n"
     ]
    }
   ],
   "source": [
    "Bow4 = Bow_transformer.transform([message4])\n",
    "print(Bow4)\n",
    "print('\\n')\n",
    "print(Bow4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso significa que existem sete palavras únicas na mensagem número 4 (depois de remover stopwords). Dois deles aparecem duas vezes, o resto apenas uma vez. Vamos em frente e verifique e confirme quais aparecem duas vezes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U\n",
      "say\n"
     ]
    }
   ],
   "source": [
    "print(Bow_transformer.get_feature_names()[4068])\n",
    "print(Bow_transformer.get_feature_names()[9554])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already\n",
      "caken\n",
      "dun\n",
      "early\n",
      "hor\n"
     ]
    }
   ],
   "source": [
    "# Agora as que aparecem apenas uma vez\n",
    "\n",
    "print(Bow_transformer.get_feature_names()[4629])\n",
    "print(Bow_transformer.get_feature_names()[5267])\n",
    "print(Bow_transformer.get_feature_names()[6204])\n",
    "print(Bow_transformer.get_feature_names()[6222])\n",
    "print(Bow_transformer.get_feature_names()[7186])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos usar *.transform * em nosso objeto transformado Bag-of-Words (bow) e transformar todo o DataFrame de mensagens numa enorme matriz esparsa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Messages_Bow = Bow_transformer.transform(Messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of our Sparse Matrix:  (5572, 11425)\n",
      "Amount of Non-zero occurrences:  50548\n"
     ]
    }
   ],
   "source": [
    "print('Shape of our Sparse Matrix: ', Messages_Bow.shape)\n",
    "print('Amount of Non-zero occurrences: ', Messages_Bow.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vejamos o grau de sparcity da nossa matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 0.07940295412668218\n"
     ]
    }
   ],
   "source": [
    "Sparsity = (100.0 * Messages_Bow.nnz / (Messages_Bow.shape[0] * Messages_Bow.shape[1]))\n",
    "print('Sparsity: {}'.format(Sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a contagem, o termo ponderação e normalização pode ser feito com [TF-IDF](http://en.wikipedia.org/wiki/Tf%E2%80%93idf), usando o `TfidfTransformer` do scikit-learn.\n",
    "\n",
    "____\n",
    "### Então, o que é TF-IDF?\n",
    "TF-IDF significa *term frequency-inverse document frequency* (frequência do termo - inverso da frequência nos documentos), e o peso tf-idf é um peso freqüentemente usado na recuperação de informações e na mineração de texto. Esse peso é uma medida estatística usada para avaliar a importância de uma palavra em um documento, coleção ou corpus. A importância aumenta proporcionalmente ao número de vezes que uma palavra aparece no documento, mas é compensada pela freqüência da palavra no corpus. As variações do esquema de ponderação tf-idf são frequentemente utilizadas pelos motores de busca como uma ferramenta central na pontuação e classificação da relevância de um documento, dada uma consulta do usuário.\n",
    "\n",
    "Uma das funções de classificação mais simples é calculada somando o tf-idf para cada termo de consulta; Muitas funções de classificação mais sofisticadas são variantes desse modelo simples.\n",
    "\n",
    "Normalmente, o peso de tf-idf é composto por dois termos: o primeiro calcula a Freqüência do termo normalizada (TF), ou seja, o número de vezes que uma palavra aparece em um documento, dividido pelo número total de palavras nesse documento; O segundo termo é a Freqüência do Documento Inverso (IDF), calculado como o logaritmo do número de documentos no corpus dividido pela quantidade de documentos onde o termo específico aparece.\n",
    "\n",
    "** TF: Frequência do termo **, que mede a frequência com que ocorre um termo em um documento. Uma vez que cada documento é diferente em comprimento, é possível que um termo apareça muito mais vezes em documentos longos do que os mais curtos. Assim, o termo freqüência é freqüentemente dividido pelo comprimento do documento (também conhecido como o número total de termos no documento) como forma de normalização:\n",
    "\n",
    "* TF (t) = (Número de vezes que o termo t aparece em um documento) / (Número total de termos no documento). *\n",
    "\n",
    "** IDF: Freqüência do Documento Inverso **, que mede o quão importante é um termo. Ao computar TF, todos os termos são considerados igualmente importantes. No entanto, é sabido que certos termos, como \"is\", \"of\", e \"that\", podem aparecer muitas vezes, mas têm pouca importância. Assim, precisamos pesar os termos freqüentes, enquanto aumentamos as raras, ao computar o seguinte:\n",
    "\n",
    "* IDF (t) = log_e (Número total de documentos / Número de documentos com termo t nele). *\n",
    "\n",
    "Veja abaixo um exemplo simples.\n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "Considere um documento contendo 100 palavras em que a palavra gato aparece 3 vezes.\n",
    "\n",
    "O termo frequência (isto é, tf) para gato é então (3/100) = 0,03. Agora, suponha que temos 10 milhões de documentos e a palavra gato aparece em mil desses. Então, a frequência inversa do documento (isto é, idf) é calculada como log (10 000 000/1000) = 4. Assim, o peso Tf-idf é o produto dessas quantidades: 0,03 * 4 = 0,12.\n",
    "____\n",
    "\n",
    "Avançemos e vejamos como podemos fazer isso no SciKit Learn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso do TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos agora instanciar a classe TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(Messages_Bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9554)\t0.5385626262927564\n",
      "  (0, 7186)\t0.4389365653379857\n",
      "  (0, 6222)\t0.3187216892949149\n",
      "  (0, 6204)\t0.29953799723697416\n",
      "  (0, 5261)\t0.29729957405868723\n",
      "  (0, 4629)\t0.26619801906087187\n",
      "  (0, 4068)\t0.40832589933384067\n"
     ]
    }
   ],
   "source": [
    "# Vamos ver um caso\n",
    "\n",
    "tfidf4 = tfidf_transformer.transform(Bow4)\n",
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos prosseguir e verificamos qual é o IDF (frequência do documento inverso) da palavra `` u '' e da palavra '' university ''?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como calcular a representação idf de uma palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.527076498901426\n"
     ]
    }
   ],
   "source": [
    "# Primeiro copiamos o nosso transformador, depois aplicamos idf passando uma lista com o que queremos transformar\n",
    "# depois pegamos o transformador inicial aplicando a opção vocabulary com uma lista que inclui a \n",
    "# palavra que estamos analisando\n",
    "print(tfidf_transformer.idf_[Bow_transformer.vocabulary_['university']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2800524267409408\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_transformer.idf_[Bow_transformer.vocabulary_['u']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para transformar todo o bag-of-words em corpus TF-IDF de uma só vez:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Messages_tfidf = tfidf_transformer.transform(Messages_Bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 11425)\n"
     ]
    }
   ],
   "source": [
    "print(Messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há muitas maneiras pelas quais os dados podem ser pré-processados e vetorizados. Essas etapas envolvem engenharia de recursos e construção de um \"pipeline\". Eu encorajo você a verificar a documentação do SciKit Learn sobre como lidar com dados de texto, bem como a ampla coleção de artigos e livros disponíveis sobre o tema geral da NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando um modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com mensagens representadas como vetores, podemos finalmente treinar nosso classificador de spam / ham. Agora, podemos realmente usar quase qualquer tipo de algoritmos de classificação. Para uma [variedade de razões](http://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn-note07-2up.pdf), o algoritmo do classificador Naive Bayes é uma boa escolha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós estaremos usando scikit-learn aqui, escolhendo o classificador [Naive Bayes](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) para começar com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para fitar, temos que o nosso X é o Messages_tfidf e o Y é Messages['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detect_model = MultinomialNB().fit(Messages_tfidf, Messages['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar classificar nossa única mensagem aleatória e verificar como performamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  ham\n",
      "Expected:  ham\n"
     ]
    }
   ],
   "source": [
    "print('Predicted: ', spam_detect_model.predict(tfidf4)[0])\n",
    "print('Expected: ', Messages.label[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantástico! Nós desenvolvemos um modelo que pode tentar prever a classificação de spam vs. classificação de presunto!\n",
    "\n",
    "## Parte 6: Avaliação do Modelo\n",
    "Agora, queremos determinar o quão bem o nosso modelo irá performar em geral em todo o conjunto de dados. Comecemos obtendo todas as previsões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'spam' ... 'ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "all_predictions = spam_detect_model.predict(Messages_tfidf)\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar o relatório de classificação incorporado do SciKit Learn, que retorna [precisão, recall,](https://en.wikipedia.org/wiki/Precision_and_recall) [f1-score](https://en.wikipedia.org/ wiki / F1_score) e uma coluna de suporte (significando quantos casos suportaram essa classificação). Confira os links para informações mais detalhadas sobre cada uma dessas métricas e a figura abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png' width=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      4825\n",
      "        spam       1.00      0.85      0.92       747\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      5572\n",
      "   macro avg       0.99      0.92      0.95      5572\n",
      "weighted avg       0.98      0.98      0.98      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(Messages['label'], all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem algumas métricas possíveis para avaliar o desempenho do modelo. O que é o mais importante depende da tarefa e dos efeitos comerciais das decisões baseadas no modelo. Por exemplo, o custo de prever \"spam\" como \"ham\" é provavelmente muito inferior ao de prever \"ham\" como \"spam\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observação: na prática não devemos usar todo o conjunto de dados para testar o nosso modelo como fizemos até aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão treino-teste\n",
    "\n",
    "Na \"avaliação\" acima, avaliamos a precisão nos mesmos dados que usamos para treinamento. ** Você nunca deve realmente avaliar no mesmo conjunto de dados em que você treina! **\n",
    "\n",
    "Essa avaliação não nos diz nada sobre o verdadeiro poder preditivo de nosso modelo. Se simplesmente nos lembrássemos de cada exemplo durante o treinamento, a precisão dos dados de treinamento seria trivialmente 100%, mesmo que não possamos classificar as novas mensagens.\n",
    "\n",
    "Uma maneira adequada é dividir os dados em um conjunto de treinamento / teste, onde o modelo só vê os ** dados de treinamento ** durante a montagem do modelo e o ajuste de parâmetros. Os ** dados de teste ** nunca são usados de forma alguma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_train, msg_test, label_train, label_test = train_test_split(Messages['message'], Messages['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando um pipeline de dados\n",
    "\n",
    "Vamos executar o nosso modelo novamente e depois prever o conjunto de testes. Usaremos os recursos [pipeline](http://scikit-learn.org/stable/modules/pipeline.html) do SciKit Learn para armazenar uma linha de fluxo de trabalho. Isso nos permitirá configurar todas as transformações que faremos aos dados para uso futuro. Vejamos um exemplo de como funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O Pipeline nos permite que criemos um fluxo de trabalho para podermos criar um algoritmo de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('Bow', CountVectorizer(analyzer=text_process)),  # Tokeniza as mensagens\n",
    "    ('tfidf', TfidfTransformer()),  # Faz a transformação em TF-IDF\n",
    "    ('classifier', MultinomialNB()),  # Define a classe que realizará nossa classificação.\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora, com o pipeline definido, podemos passar diretamente os dados de texto da mensagem e o pipeline fará o nosso pré-processamento para nós! Podemos tratá-lo como uma API modelo / estimador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('Bow', CountVectorizer(analyzer=<function text_process at 0x7f593ebbb378>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=No...f=False, use_idf=True)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(msg_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.95      0.98      1014\n",
      "        spam       0.68      1.00      0.81       101\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1115\n",
      "   macro avg       0.84      0.98      0.89      1115\n",
      "weighted avg       0.97      0.96      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Predictions, label_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a classification report for our model on a true testing set! There is a lot more to Natural Language Processing than what we've covered here, and its vast expanse of topic could fill up several college courses! I encourage you to check out the resources below for more information on NLP!\n",
    "\n",
    "Agora, temos um relatório de classificação para o nosso modelo em um verdadeiro conjunto de testes! Há muito mais para se explorar no processamento de linguagem natural além do que cobrimos aqui, e sua vasta extensão de tópicos pode preencher vários cursos universitários! Eu encorajo você a verificar os recursos abaixo para obter mais informações sobre a PNL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mais recursos\n",
    "\n",
    "Confira os links abaixo para mais informações sobre Processamento de linguagem natural:\n",
    "\n",
    "[NLTK Book Online](http://www.nltk.org/book/)\n",
    "\n",
    "[Kaggle Walkthrough](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words)\n",
    "\n",
    "[SciKit Learn's Tutorial](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos testar os resultados usando mais um método de classificador: o RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_RFC = Pipeline([\n",
    "    ('Bow', CountVectorizer(analyzer=text_process)),  # Tokeniza as mensagens\n",
    "    ('tfidf', TfidfTransformer()),  # Faz a transformação em TF-IDF\n",
    "    ('classifier', RandomForestClassifier()),  # Define a classe que realizará nossa classificação.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/Python/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('Bow', CountVectorizer(analyzer=<function text_process at 0x7f593ebbb378>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=No...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_RFC.fit(msg_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions_RFC = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.95      0.98      1014\n",
      "        spam       0.68      1.00      0.81       101\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1115\n",
      "   macro avg       0.84      0.98      0.89      1115\n",
      "weighted avg       0.97      0.96      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Predictions_RFC, label_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notemos que o método RandomForestClassifier foi tão bom quanto o método classificador MultinomialNB."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
