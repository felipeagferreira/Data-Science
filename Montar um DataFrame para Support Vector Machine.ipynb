{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines com Python\n",
    "\n",
    "Bem vindo ao notebook sobre Support Vector Machines com Python! Lembre-se de se consultar o vídeo para obter informações completas sobre o código aqui!\n",
    "\n",
    "## Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obter dados\n",
    "\n",
    "Usaremos o conjunto de dados de câncer de mama incorporado da Scikit Learn. Podemos obter com a função load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Este conjunto de dados é apresentado em uma forma de dicionário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para acessar a descrição com a finalidade de ver melhor esses dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry \n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "        13 is Radius SE, field 23 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "print(cancer['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Informações técnicas sobre a possibilidade de ter ou não câncer\n",
    "cancer['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "        1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "        8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "        8.758e-02],\n",
       "       ...,\n",
       "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "        7.820e-02],\n",
       "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "        1.240e-01],\n",
       "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "        7.039e-02]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(cancer['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cancer['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando o DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para podermos criar um modelo de machine learning, devemos primeiro configurar o DataFrame. Usaremos como parâmetro dos dados o 'data' e como colunas o 'feature names' do \"cancer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancer = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 30 columns):\n",
      "mean radius                569 non-null float64\n",
      "mean texture               569 non-null float64\n",
      "mean perimeter             569 non-null float64\n",
      "mean area                  569 non-null float64\n",
      "mean smoothness            569 non-null float64\n",
      "mean compactness           569 non-null float64\n",
      "mean concavity             569 non-null float64\n",
      "mean concave points        569 non-null float64\n",
      "mean symmetry              569 non-null float64\n",
      "mean fractal dimension     569 non-null float64\n",
      "radius error               569 non-null float64\n",
      "texture error              569 non-null float64\n",
      "perimeter error            569 non-null float64\n",
      "area error                 569 non-null float64\n",
      "smoothness error           569 non-null float64\n",
      "compactness error          569 non-null float64\n",
      "concavity error            569 non-null float64\n",
      "concave points error       569 non-null float64\n",
      "symmetry error             569 non-null float64\n",
      "fractal dimension error    569 non-null float64\n",
      "worst radius               569 non-null float64\n",
      "worst texture              569 non-null float64\n",
      "worst perimeter            569 non-null float64\n",
      "worst area                 569 non-null float64\n",
      "worst smoothness           569 non-null float64\n",
      "worst compactness          569 non-null float64\n",
      "worst concavity            569 non-null float64\n",
      "worst concave points       569 non-null float64\n",
      "worst symmetry             569 non-null float64\n",
      "worst fractal dimension    569 non-null float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 133.4 KB\n"
     ]
    }
   ],
   "source": [
    "# Agora temos todos os nossos dados colocados dentro de um DataFrame\n",
    "df_cancer.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Este foi o DataFrame dos parâmetros X do nosso modelo de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension           ...             worst radius  \\\n",
       "0                 0.07871           ...                    25.38   \n",
       "1                 0.05667           ...                    24.99   \n",
       "2                 0.05999           ...                    23.57   \n",
       "3                 0.09744           ...                    14.91   \n",
       "4                 0.05883           ...                    22.54   \n",
       "\n",
       "   worst texture  worst perimeter  worst area  worst smoothness  \\\n",
       "0          17.33           184.60      2019.0            0.1622   \n",
       "1          23.41           158.80      1956.0            0.1238   \n",
       "2          25.53           152.50      1709.0            0.1444   \n",
       "3          26.50            98.87       567.7            0.2098   \n",
       "4          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   worst compactness  worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   worst fractal dimension  \n",
       "0                  0.11890  \n",
       "1                  0.08902  \n",
       "2                  0.08758  \n",
       "3                  0.17300  \n",
       "4                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mas lembrando que SVM é um algoritmo de ML baseado em aprendizado supervisionado, precisamos ainda criar o DataFrame de target para dar as respostas Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cancer['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos nomear a coluna do nosso DataFrame dos Y de 'Cancer'.\n",
    "df_target = pd.DataFrame(cancer['target'], columns=['Cancer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora vamos verificar o DataFrame dos Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cancer\n",
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "5         0\n",
       "6         0\n",
       "7         0\n",
       "8         0\n",
       "9         0\n",
       "10        0\n",
       "11        0\n",
       "12        0\n",
       "13        0\n",
       "14        0\n",
       "15        0\n",
       "16        0\n",
       "17        0\n",
       "18        0\n",
       "19        1\n",
       "20        1\n",
       "21        1\n",
       "22        0\n",
       "23        0\n",
       "24        0\n",
       "25        0\n",
       "26        0\n",
       "27        0\n",
       "28        0\n",
       "29        0\n",
       "..      ...\n",
       "539       1\n",
       "540       1\n",
       "541       1\n",
       "542       1\n",
       "543       1\n",
       "544       1\n",
       "545       1\n",
       "546       1\n",
       "547       1\n",
       "548       1\n",
       "549       1\n",
       "550       1\n",
       "551       1\n",
       "552       1\n",
       "553       1\n",
       "554       1\n",
       "555       1\n",
       "556       1\n",
       "557       1\n",
       "558       1\n",
       "559       1\n",
       "560       1\n",
       "561       1\n",
       "562       0\n",
       "563       0\n",
       "564       0\n",
       "565       0\n",
       "566       0\n",
       "567       0\n",
       "568       1\n",
       "\n",
       "[569 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de dados exploratórios\n",
    "\n",
    "Nós ignoraremos a parte Data Vizualization para esta leitura, pois existem tantos parâmetros que são difíceis de interpretar se você não tiver conhecimento e domínio de câncer ou células tumorais. No seu projeto você terá mais oportunidades para visualizar os dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divisão treino-teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ravel(df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocamos o np.ravel no Y para converter os dados Y em array\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_cancer, np.ravel(df_target), test_size = 0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>13.010</td>\n",
       "      <td>22.22</td>\n",
       "      <td>82.01</td>\n",
       "      <td>526.4</td>\n",
       "      <td>0.06251</td>\n",
       "      <td>0.01938</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.1395</td>\n",
       "      <td>0.05234</td>\n",
       "      <td>...</td>\n",
       "      <td>14.000</td>\n",
       "      <td>29.02</td>\n",
       "      <td>88.18</td>\n",
       "      <td>608.8</td>\n",
       "      <td>0.08125</td>\n",
       "      <td>0.03432</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.2295</td>\n",
       "      <td>0.05843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>14.690</td>\n",
       "      <td>13.98</td>\n",
       "      <td>98.22</td>\n",
       "      <td>656.1</td>\n",
       "      <td>0.10310</td>\n",
       "      <td>0.18360</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.2086</td>\n",
       "      <td>0.07406</td>\n",
       "      <td>...</td>\n",
       "      <td>16.460</td>\n",
       "      <td>18.34</td>\n",
       "      <td>114.10</td>\n",
       "      <td>809.2</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.36350</td>\n",
       "      <td>0.321900</td>\n",
       "      <td>0.110800</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.09208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>14.710</td>\n",
       "      <td>21.59</td>\n",
       "      <td>95.55</td>\n",
       "      <td>656.9</td>\n",
       "      <td>0.11370</td>\n",
       "      <td>0.13650</td>\n",
       "      <td>0.129300</td>\n",
       "      <td>0.081230</td>\n",
       "      <td>0.2027</td>\n",
       "      <td>0.06758</td>\n",
       "      <td>...</td>\n",
       "      <td>17.870</td>\n",
       "      <td>30.70</td>\n",
       "      <td>115.70</td>\n",
       "      <td>985.5</td>\n",
       "      <td>0.13680</td>\n",
       "      <td>0.42900</td>\n",
       "      <td>0.358700</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>0.3698</td>\n",
       "      <td>0.10940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>15.050</td>\n",
       "      <td>19.07</td>\n",
       "      <td>97.26</td>\n",
       "      <td>701.9</td>\n",
       "      <td>0.09215</td>\n",
       "      <td>0.08597</td>\n",
       "      <td>0.074860</td>\n",
       "      <td>0.043350</td>\n",
       "      <td>0.1561</td>\n",
       "      <td>0.05915</td>\n",
       "      <td>...</td>\n",
       "      <td>17.580</td>\n",
       "      <td>28.06</td>\n",
       "      <td>113.80</td>\n",
       "      <td>967.0</td>\n",
       "      <td>0.12460</td>\n",
       "      <td>0.21010</td>\n",
       "      <td>0.286600</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.2282</td>\n",
       "      <td>0.06954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>9.683</td>\n",
       "      <td>19.34</td>\n",
       "      <td>61.05</td>\n",
       "      <td>285.7</td>\n",
       "      <td>0.08491</td>\n",
       "      <td>0.05030</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.06235</td>\n",
       "      <td>...</td>\n",
       "      <td>10.930</td>\n",
       "      <td>25.59</td>\n",
       "      <td>69.10</td>\n",
       "      <td>364.2</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.09546</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.038460</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>0.07920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>11.630</td>\n",
       "      <td>29.29</td>\n",
       "      <td>74.87</td>\n",
       "      <td>415.1</td>\n",
       "      <td>0.09357</td>\n",
       "      <td>0.08574</td>\n",
       "      <td>0.071600</td>\n",
       "      <td>0.020170</td>\n",
       "      <td>0.1799</td>\n",
       "      <td>0.06166</td>\n",
       "      <td>...</td>\n",
       "      <td>13.120</td>\n",
       "      <td>38.81</td>\n",
       "      <td>86.04</td>\n",
       "      <td>527.8</td>\n",
       "      <td>0.14060</td>\n",
       "      <td>0.20310</td>\n",
       "      <td>0.292300</td>\n",
       "      <td>0.068350</td>\n",
       "      <td>0.2884</td>\n",
       "      <td>0.07220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>14.810</td>\n",
       "      <td>14.70</td>\n",
       "      <td>94.66</td>\n",
       "      <td>680.7</td>\n",
       "      <td>0.08472</td>\n",
       "      <td>0.05016</td>\n",
       "      <td>0.034160</td>\n",
       "      <td>0.025410</td>\n",
       "      <td>0.1659</td>\n",
       "      <td>0.05348</td>\n",
       "      <td>...</td>\n",
       "      <td>15.610</td>\n",
       "      <td>17.58</td>\n",
       "      <td>101.70</td>\n",
       "      <td>760.2</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.10110</td>\n",
       "      <td>0.110100</td>\n",
       "      <td>0.079550</td>\n",
       "      <td>0.2334</td>\n",
       "      <td>0.06142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>12.780</td>\n",
       "      <td>16.49</td>\n",
       "      <td>81.37</td>\n",
       "      <td>502.5</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.05234</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.028640</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05653</td>\n",
       "      <td>...</td>\n",
       "      <td>13.460</td>\n",
       "      <td>19.76</td>\n",
       "      <td>85.67</td>\n",
       "      <td>554.9</td>\n",
       "      <td>0.12960</td>\n",
       "      <td>0.07061</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>0.058820</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>0.06410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>11.360</td>\n",
       "      <td>17.57</td>\n",
       "      <td>72.49</td>\n",
       "      <td>399.8</td>\n",
       "      <td>0.08858</td>\n",
       "      <td>0.05313</td>\n",
       "      <td>0.027830</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.05913</td>\n",
       "      <td>...</td>\n",
       "      <td>13.050</td>\n",
       "      <td>36.32</td>\n",
       "      <td>85.07</td>\n",
       "      <td>521.3</td>\n",
       "      <td>0.14530</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.181100</td>\n",
       "      <td>0.086980</td>\n",
       "      <td>0.2973</td>\n",
       "      <td>0.07745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>12.050</td>\n",
       "      <td>22.72</td>\n",
       "      <td>78.75</td>\n",
       "      <td>447.8</td>\n",
       "      <td>0.06935</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.079430</td>\n",
       "      <td>0.029780</td>\n",
       "      <td>0.1203</td>\n",
       "      <td>0.06659</td>\n",
       "      <td>...</td>\n",
       "      <td>12.570</td>\n",
       "      <td>28.71</td>\n",
       "      <td>87.36</td>\n",
       "      <td>488.4</td>\n",
       "      <td>0.08799</td>\n",
       "      <td>0.32140</td>\n",
       "      <td>0.291200</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>0.2191</td>\n",
       "      <td>0.09349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10.950</td>\n",
       "      <td>21.35</td>\n",
       "      <td>71.90</td>\n",
       "      <td>371.1</td>\n",
       "      <td>0.12270</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.056690</td>\n",
       "      <td>0.1895</td>\n",
       "      <td>0.06870</td>\n",
       "      <td>...</td>\n",
       "      <td>12.840</td>\n",
       "      <td>35.34</td>\n",
       "      <td>87.22</td>\n",
       "      <td>514.0</td>\n",
       "      <td>0.19090</td>\n",
       "      <td>0.26980</td>\n",
       "      <td>0.402300</td>\n",
       "      <td>0.142400</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>0.09606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>18.940</td>\n",
       "      <td>21.31</td>\n",
       "      <td>123.60</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>0.09009</td>\n",
       "      <td>0.10290</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.079510</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.05461</td>\n",
       "      <td>...</td>\n",
       "      <td>24.860</td>\n",
       "      <td>26.58</td>\n",
       "      <td>165.90</td>\n",
       "      <td>1866.0</td>\n",
       "      <td>0.11930</td>\n",
       "      <td>0.23360</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.2551</td>\n",
       "      <td>0.06589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>11.410</td>\n",
       "      <td>14.92</td>\n",
       "      <td>73.53</td>\n",
       "      <td>402.0</td>\n",
       "      <td>0.09059</td>\n",
       "      <td>0.08155</td>\n",
       "      <td>0.061810</td>\n",
       "      <td>0.023610</td>\n",
       "      <td>0.1167</td>\n",
       "      <td>0.06217</td>\n",
       "      <td>...</td>\n",
       "      <td>12.370</td>\n",
       "      <td>17.70</td>\n",
       "      <td>79.12</td>\n",
       "      <td>467.2</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.16100</td>\n",
       "      <td>0.164800</td>\n",
       "      <td>0.062960</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.07427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>11.250</td>\n",
       "      <td>14.78</td>\n",
       "      <td>71.38</td>\n",
       "      <td>390.0</td>\n",
       "      <td>0.08306</td>\n",
       "      <td>0.04458</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>0.1773</td>\n",
       "      <td>0.06081</td>\n",
       "      <td>...</td>\n",
       "      <td>12.760</td>\n",
       "      <td>22.06</td>\n",
       "      <td>82.08</td>\n",
       "      <td>492.7</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.09794</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.016670</td>\n",
       "      <td>0.2815</td>\n",
       "      <td>0.07418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>12.200</td>\n",
       "      <td>15.21</td>\n",
       "      <td>78.01</td>\n",
       "      <td>457.9</td>\n",
       "      <td>0.08673</td>\n",
       "      <td>0.06545</td>\n",
       "      <td>0.019940</td>\n",
       "      <td>0.016920</td>\n",
       "      <td>0.1638</td>\n",
       "      <td>0.06129</td>\n",
       "      <td>...</td>\n",
       "      <td>13.750</td>\n",
       "      <td>21.38</td>\n",
       "      <td>91.11</td>\n",
       "      <td>583.1</td>\n",
       "      <td>0.12560</td>\n",
       "      <td>0.19280</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>0.055560</td>\n",
       "      <td>0.2661</td>\n",
       "      <td>0.07961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>15.490</td>\n",
       "      <td>19.97</td>\n",
       "      <td>102.40</td>\n",
       "      <td>744.7</td>\n",
       "      <td>0.11600</td>\n",
       "      <td>0.15620</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.091130</td>\n",
       "      <td>0.1929</td>\n",
       "      <td>0.06744</td>\n",
       "      <td>...</td>\n",
       "      <td>21.200</td>\n",
       "      <td>29.41</td>\n",
       "      <td>142.10</td>\n",
       "      <td>1359.0</td>\n",
       "      <td>0.16810</td>\n",
       "      <td>0.39130</td>\n",
       "      <td>0.555300</td>\n",
       "      <td>0.212100</td>\n",
       "      <td>0.3187</td>\n",
       "      <td>0.10190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>10.800</td>\n",
       "      <td>21.98</td>\n",
       "      <td>68.79</td>\n",
       "      <td>359.9</td>\n",
       "      <td>0.08801</td>\n",
       "      <td>0.05743</td>\n",
       "      <td>0.036140</td>\n",
       "      <td>0.014040</td>\n",
       "      <td>0.2016</td>\n",
       "      <td>0.05977</td>\n",
       "      <td>...</td>\n",
       "      <td>12.760</td>\n",
       "      <td>32.04</td>\n",
       "      <td>83.69</td>\n",
       "      <td>489.5</td>\n",
       "      <td>0.13030</td>\n",
       "      <td>0.16960</td>\n",
       "      <td>0.192700</td>\n",
       "      <td>0.074850</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>0.07662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>13.280</td>\n",
       "      <td>20.28</td>\n",
       "      <td>87.32</td>\n",
       "      <td>545.2</td>\n",
       "      <td>0.10410</td>\n",
       "      <td>0.14360</td>\n",
       "      <td>0.098470</td>\n",
       "      <td>0.061580</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.06782</td>\n",
       "      <td>...</td>\n",
       "      <td>17.380</td>\n",
       "      <td>28.00</td>\n",
       "      <td>113.10</td>\n",
       "      <td>907.2</td>\n",
       "      <td>0.15300</td>\n",
       "      <td>0.37240</td>\n",
       "      <td>0.366400</td>\n",
       "      <td>0.149200</td>\n",
       "      <td>0.3739</td>\n",
       "      <td>0.10270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>14.250</td>\n",
       "      <td>21.72</td>\n",
       "      <td>93.63</td>\n",
       "      <td>633.0</td>\n",
       "      <td>0.09823</td>\n",
       "      <td>0.10980</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>0.055980</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.06125</td>\n",
       "      <td>...</td>\n",
       "      <td>15.890</td>\n",
       "      <td>30.36</td>\n",
       "      <td>116.20</td>\n",
       "      <td>799.6</td>\n",
       "      <td>0.14460</td>\n",
       "      <td>0.42380</td>\n",
       "      <td>0.518600</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.3591</td>\n",
       "      <td>0.10140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>15.270</td>\n",
       "      <td>12.91</td>\n",
       "      <td>98.17</td>\n",
       "      <td>725.5</td>\n",
       "      <td>0.08182</td>\n",
       "      <td>0.06230</td>\n",
       "      <td>0.058920</td>\n",
       "      <td>0.031570</td>\n",
       "      <td>0.1359</td>\n",
       "      <td>0.05526</td>\n",
       "      <td>...</td>\n",
       "      <td>17.380</td>\n",
       "      <td>15.92</td>\n",
       "      <td>113.70</td>\n",
       "      <td>932.7</td>\n",
       "      <td>0.12220</td>\n",
       "      <td>0.21860</td>\n",
       "      <td>0.296200</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.07474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>16.030</td>\n",
       "      <td>15.51</td>\n",
       "      <td>105.80</td>\n",
       "      <td>793.2</td>\n",
       "      <td>0.09491</td>\n",
       "      <td>0.13710</td>\n",
       "      <td>0.120400</td>\n",
       "      <td>0.070410</td>\n",
       "      <td>0.1782</td>\n",
       "      <td>0.05976</td>\n",
       "      <td>...</td>\n",
       "      <td>18.760</td>\n",
       "      <td>21.98</td>\n",
       "      <td>124.30</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>0.14350</td>\n",
       "      <td>0.44780</td>\n",
       "      <td>0.495600</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.3019</td>\n",
       "      <td>0.09124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>11.710</td>\n",
       "      <td>15.45</td>\n",
       "      <td>75.03</td>\n",
       "      <td>420.3</td>\n",
       "      <td>0.11500</td>\n",
       "      <td>0.07281</td>\n",
       "      <td>0.040060</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.2009</td>\n",
       "      <td>0.06506</td>\n",
       "      <td>...</td>\n",
       "      <td>13.060</td>\n",
       "      <td>18.16</td>\n",
       "      <td>84.16</td>\n",
       "      <td>516.4</td>\n",
       "      <td>0.14600</td>\n",
       "      <td>0.11150</td>\n",
       "      <td>0.108700</td>\n",
       "      <td>0.078640</td>\n",
       "      <td>0.2765</td>\n",
       "      <td>0.07806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>10.290</td>\n",
       "      <td>27.61</td>\n",
       "      <td>65.67</td>\n",
       "      <td>321.4</td>\n",
       "      <td>0.09030</td>\n",
       "      <td>0.07658</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.06127</td>\n",
       "      <td>...</td>\n",
       "      <td>10.840</td>\n",
       "      <td>34.91</td>\n",
       "      <td>69.57</td>\n",
       "      <td>357.6</td>\n",
       "      <td>0.13840</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.091270</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.08283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>12.320</td>\n",
       "      <td>12.39</td>\n",
       "      <td>78.85</td>\n",
       "      <td>464.1</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.06981</td>\n",
       "      <td>0.039870</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.1959</td>\n",
       "      <td>0.05955</td>\n",
       "      <td>...</td>\n",
       "      <td>13.500</td>\n",
       "      <td>15.64</td>\n",
       "      <td>86.97</td>\n",
       "      <td>549.1</td>\n",
       "      <td>0.13850</td>\n",
       "      <td>0.12660</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>0.093910</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.06771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>16.170</td>\n",
       "      <td>16.07</td>\n",
       "      <td>106.30</td>\n",
       "      <td>788.5</td>\n",
       "      <td>0.09880</td>\n",
       "      <td>0.14380</td>\n",
       "      <td>0.066510</td>\n",
       "      <td>0.053970</td>\n",
       "      <td>0.1990</td>\n",
       "      <td>0.06572</td>\n",
       "      <td>...</td>\n",
       "      <td>16.970</td>\n",
       "      <td>19.14</td>\n",
       "      <td>113.10</td>\n",
       "      <td>861.5</td>\n",
       "      <td>0.12350</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>0.125100</td>\n",
       "      <td>0.3153</td>\n",
       "      <td>0.08960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>12.900</td>\n",
       "      <td>15.92</td>\n",
       "      <td>83.74</td>\n",
       "      <td>512.2</td>\n",
       "      <td>0.08677</td>\n",
       "      <td>0.09509</td>\n",
       "      <td>0.048940</td>\n",
       "      <td>0.030880</td>\n",
       "      <td>0.1778</td>\n",
       "      <td>0.06235</td>\n",
       "      <td>...</td>\n",
       "      <td>14.480</td>\n",
       "      <td>21.82</td>\n",
       "      <td>97.17</td>\n",
       "      <td>643.8</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.25480</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.3549</td>\n",
       "      <td>0.08118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>12.540</td>\n",
       "      <td>18.07</td>\n",
       "      <td>79.42</td>\n",
       "      <td>491.9</td>\n",
       "      <td>0.07436</td>\n",
       "      <td>0.02650</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.005449</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05185</td>\n",
       "      <td>...</td>\n",
       "      <td>13.720</td>\n",
       "      <td>20.98</td>\n",
       "      <td>86.82</td>\n",
       "      <td>585.7</td>\n",
       "      <td>0.09293</td>\n",
       "      <td>0.04327</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>0.016350</td>\n",
       "      <td>0.2233</td>\n",
       "      <td>0.05521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>21.100</td>\n",
       "      <td>20.52</td>\n",
       "      <td>138.10</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>0.09684</td>\n",
       "      <td>0.11750</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.115500</td>\n",
       "      <td>0.1554</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>...</td>\n",
       "      <td>25.680</td>\n",
       "      <td>32.07</td>\n",
       "      <td>168.20</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>0.13680</td>\n",
       "      <td>0.31010</td>\n",
       "      <td>0.439900</td>\n",
       "      <td>0.228000</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.07425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>12.340</td>\n",
       "      <td>12.27</td>\n",
       "      <td>78.94</td>\n",
       "      <td>468.5</td>\n",
       "      <td>0.09003</td>\n",
       "      <td>0.06307</td>\n",
       "      <td>0.029580</td>\n",
       "      <td>0.026470</td>\n",
       "      <td>0.1689</td>\n",
       "      <td>0.05808</td>\n",
       "      <td>...</td>\n",
       "      <td>13.610</td>\n",
       "      <td>19.27</td>\n",
       "      <td>87.22</td>\n",
       "      <td>564.9</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.20740</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.3110</td>\n",
       "      <td>0.07592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>15.000</td>\n",
       "      <td>15.51</td>\n",
       "      <td>97.45</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.08371</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.065050</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.1881</td>\n",
       "      <td>0.05907</td>\n",
       "      <td>...</td>\n",
       "      <td>16.410</td>\n",
       "      <td>19.31</td>\n",
       "      <td>114.20</td>\n",
       "      <td>808.2</td>\n",
       "      <td>0.11360</td>\n",
       "      <td>0.36270</td>\n",
       "      <td>0.340200</td>\n",
       "      <td>0.137900</td>\n",
       "      <td>0.2954</td>\n",
       "      <td>0.08362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>10.970</td>\n",
       "      <td>17.20</td>\n",
       "      <td>71.73</td>\n",
       "      <td>371.5</td>\n",
       "      <td>0.08915</td>\n",
       "      <td>0.11130</td>\n",
       "      <td>0.094570</td>\n",
       "      <td>0.036130</td>\n",
       "      <td>0.1489</td>\n",
       "      <td>0.06640</td>\n",
       "      <td>...</td>\n",
       "      <td>12.360</td>\n",
       "      <td>26.87</td>\n",
       "      <td>90.14</td>\n",
       "      <td>476.4</td>\n",
       "      <td>0.13910</td>\n",
       "      <td>0.40820</td>\n",
       "      <td>0.477900</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.09532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>12.870</td>\n",
       "      <td>19.54</td>\n",
       "      <td>82.67</td>\n",
       "      <td>509.2</td>\n",
       "      <td>0.09136</td>\n",
       "      <td>0.07883</td>\n",
       "      <td>0.017970</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.06347</td>\n",
       "      <td>...</td>\n",
       "      <td>14.450</td>\n",
       "      <td>24.38</td>\n",
       "      <td>95.14</td>\n",
       "      <td>626.9</td>\n",
       "      <td>0.12140</td>\n",
       "      <td>0.16520</td>\n",
       "      <td>0.071270</td>\n",
       "      <td>0.063840</td>\n",
       "      <td>0.3313</td>\n",
       "      <td>0.07735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>12.750</td>\n",
       "      <td>16.70</td>\n",
       "      <td>82.51</td>\n",
       "      <td>493.8</td>\n",
       "      <td>0.11250</td>\n",
       "      <td>0.11170</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.029950</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.06623</td>\n",
       "      <td>...</td>\n",
       "      <td>14.450</td>\n",
       "      <td>21.74</td>\n",
       "      <td>93.63</td>\n",
       "      <td>624.1</td>\n",
       "      <td>0.14750</td>\n",
       "      <td>0.19790</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>0.080450</td>\n",
       "      <td>0.3071</td>\n",
       "      <td>0.08557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>12.890</td>\n",
       "      <td>15.70</td>\n",
       "      <td>84.08</td>\n",
       "      <td>516.6</td>\n",
       "      <td>0.07818</td>\n",
       "      <td>0.09580</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.1432</td>\n",
       "      <td>0.05935</td>\n",
       "      <td>...</td>\n",
       "      <td>13.900</td>\n",
       "      <td>19.69</td>\n",
       "      <td>92.12</td>\n",
       "      <td>595.6</td>\n",
       "      <td>0.09926</td>\n",
       "      <td>0.23170</td>\n",
       "      <td>0.334400</td>\n",
       "      <td>0.101700</td>\n",
       "      <td>0.1999</td>\n",
       "      <td>0.07127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>14.420</td>\n",
       "      <td>19.77</td>\n",
       "      <td>94.48</td>\n",
       "      <td>642.5</td>\n",
       "      <td>0.09752</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.093880</td>\n",
       "      <td>0.058390</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>0.06390</td>\n",
       "      <td>...</td>\n",
       "      <td>16.330</td>\n",
       "      <td>30.86</td>\n",
       "      <td>109.50</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.14310</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.319400</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.2718</td>\n",
       "      <td>0.09353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>17.190</td>\n",
       "      <td>22.07</td>\n",
       "      <td>111.60</td>\n",
       "      <td>928.3</td>\n",
       "      <td>0.09726</td>\n",
       "      <td>0.08995</td>\n",
       "      <td>0.090610</td>\n",
       "      <td>0.065270</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.05580</td>\n",
       "      <td>...</td>\n",
       "      <td>21.580</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.50</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>0.15580</td>\n",
       "      <td>0.25670</td>\n",
       "      <td>0.388900</td>\n",
       "      <td>0.198400</td>\n",
       "      <td>0.3216</td>\n",
       "      <td>0.07570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>17.460</td>\n",
       "      <td>39.28</td>\n",
       "      <td>113.40</td>\n",
       "      <td>920.6</td>\n",
       "      <td>0.09812</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.088110</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05966</td>\n",
       "      <td>...</td>\n",
       "      <td>22.510</td>\n",
       "      <td>44.87</td>\n",
       "      <td>141.20</td>\n",
       "      <td>1408.0</td>\n",
       "      <td>0.13650</td>\n",
       "      <td>0.37350</td>\n",
       "      <td>0.324100</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>0.2853</td>\n",
       "      <td>0.08496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>13.210</td>\n",
       "      <td>25.25</td>\n",
       "      <td>84.10</td>\n",
       "      <td>537.9</td>\n",
       "      <td>0.08791</td>\n",
       "      <td>0.05205</td>\n",
       "      <td>0.027720</td>\n",
       "      <td>0.020680</td>\n",
       "      <td>0.1619</td>\n",
       "      <td>0.05584</td>\n",
       "      <td>...</td>\n",
       "      <td>14.350</td>\n",
       "      <td>34.23</td>\n",
       "      <td>91.29</td>\n",
       "      <td>632.9</td>\n",
       "      <td>0.12890</td>\n",
       "      <td>0.10630</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.060050</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.06788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>23.090</td>\n",
       "      <td>19.83</td>\n",
       "      <td>152.10</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>0.09342</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.05484</td>\n",
       "      <td>...</td>\n",
       "      <td>30.790</td>\n",
       "      <td>23.87</td>\n",
       "      <td>211.50</td>\n",
       "      <td>2782.0</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.36250</td>\n",
       "      <td>0.379400</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.2908</td>\n",
       "      <td>0.07277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>13.110</td>\n",
       "      <td>15.56</td>\n",
       "      <td>87.21</td>\n",
       "      <td>530.2</td>\n",
       "      <td>0.13980</td>\n",
       "      <td>0.17650</td>\n",
       "      <td>0.207100</td>\n",
       "      <td>0.096010</td>\n",
       "      <td>0.1925</td>\n",
       "      <td>0.07692</td>\n",
       "      <td>...</td>\n",
       "      <td>16.310</td>\n",
       "      <td>22.40</td>\n",
       "      <td>106.40</td>\n",
       "      <td>827.2</td>\n",
       "      <td>0.18620</td>\n",
       "      <td>0.40990</td>\n",
       "      <td>0.637600</td>\n",
       "      <td>0.198600</td>\n",
       "      <td>0.3147</td>\n",
       "      <td>0.14050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>19.680</td>\n",
       "      <td>21.68</td>\n",
       "      <td>129.90</td>\n",
       "      <td>1194.0</td>\n",
       "      <td>0.09797</td>\n",
       "      <td>0.13390</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.2082</td>\n",
       "      <td>0.05715</td>\n",
       "      <td>...</td>\n",
       "      <td>22.750</td>\n",
       "      <td>34.66</td>\n",
       "      <td>157.60</td>\n",
       "      <td>1540.0</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.34580</td>\n",
       "      <td>0.473400</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>0.4045</td>\n",
       "      <td>0.07918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>13.530</td>\n",
       "      <td>10.94</td>\n",
       "      <td>87.91</td>\n",
       "      <td>559.2</td>\n",
       "      <td>0.12910</td>\n",
       "      <td>0.10470</td>\n",
       "      <td>0.068770</td>\n",
       "      <td>0.065560</td>\n",
       "      <td>0.2403</td>\n",
       "      <td>0.06641</td>\n",
       "      <td>...</td>\n",
       "      <td>14.080</td>\n",
       "      <td>12.49</td>\n",
       "      <td>91.36</td>\n",
       "      <td>605.5</td>\n",
       "      <td>0.14510</td>\n",
       "      <td>0.13790</td>\n",
       "      <td>0.085390</td>\n",
       "      <td>0.074070</td>\n",
       "      <td>0.2710</td>\n",
       "      <td>0.07191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>11.890</td>\n",
       "      <td>17.36</td>\n",
       "      <td>76.20</td>\n",
       "      <td>435.6</td>\n",
       "      <td>0.12250</td>\n",
       "      <td>0.07210</td>\n",
       "      <td>0.059290</td>\n",
       "      <td>0.074040</td>\n",
       "      <td>0.2015</td>\n",
       "      <td>0.05875</td>\n",
       "      <td>...</td>\n",
       "      <td>12.400</td>\n",
       "      <td>18.99</td>\n",
       "      <td>79.46</td>\n",
       "      <td>472.4</td>\n",
       "      <td>0.13590</td>\n",
       "      <td>0.08368</td>\n",
       "      <td>0.071530</td>\n",
       "      <td>0.089460</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.06033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>11.520</td>\n",
       "      <td>14.93</td>\n",
       "      <td>73.87</td>\n",
       "      <td>406.3</td>\n",
       "      <td>0.10130</td>\n",
       "      <td>0.07808</td>\n",
       "      <td>0.043280</td>\n",
       "      <td>0.029290</td>\n",
       "      <td>0.1883</td>\n",
       "      <td>0.06168</td>\n",
       "      <td>...</td>\n",
       "      <td>12.650</td>\n",
       "      <td>21.19</td>\n",
       "      <td>80.88</td>\n",
       "      <td>491.8</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.15820</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.096080</td>\n",
       "      <td>0.2664</td>\n",
       "      <td>0.07809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>12.700</td>\n",
       "      <td>12.17</td>\n",
       "      <td>80.88</td>\n",
       "      <td>495.0</td>\n",
       "      <td>0.08785</td>\n",
       "      <td>0.05794</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.024020</td>\n",
       "      <td>0.1583</td>\n",
       "      <td>0.06275</td>\n",
       "      <td>...</td>\n",
       "      <td>13.650</td>\n",
       "      <td>16.92</td>\n",
       "      <td>88.12</td>\n",
       "      <td>566.9</td>\n",
       "      <td>0.13140</td>\n",
       "      <td>0.16070</td>\n",
       "      <td>0.093850</td>\n",
       "      <td>0.082240</td>\n",
       "      <td>0.2775</td>\n",
       "      <td>0.09464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>8.618</td>\n",
       "      <td>11.79</td>\n",
       "      <td>54.34</td>\n",
       "      <td>224.5</td>\n",
       "      <td>0.09752</td>\n",
       "      <td>0.05272</td>\n",
       "      <td>0.020610</td>\n",
       "      <td>0.007799</td>\n",
       "      <td>0.1683</td>\n",
       "      <td>0.07187</td>\n",
       "      <td>...</td>\n",
       "      <td>9.507</td>\n",
       "      <td>15.40</td>\n",
       "      <td>59.90</td>\n",
       "      <td>274.9</td>\n",
       "      <td>0.17330</td>\n",
       "      <td>0.12390</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.044190</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>0.09026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>15.190</td>\n",
       "      <td>13.21</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.8</td>\n",
       "      <td>0.07963</td>\n",
       "      <td>0.06934</td>\n",
       "      <td>0.033930</td>\n",
       "      <td>0.026570</td>\n",
       "      <td>0.1721</td>\n",
       "      <td>0.05544</td>\n",
       "      <td>...</td>\n",
       "      <td>16.200</td>\n",
       "      <td>15.73</td>\n",
       "      <td>104.50</td>\n",
       "      <td>819.1</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.17370</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.081780</td>\n",
       "      <td>0.2487</td>\n",
       "      <td>0.06766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>11.710</td>\n",
       "      <td>16.67</td>\n",
       "      <td>74.72</td>\n",
       "      <td>423.6</td>\n",
       "      <td>0.10510</td>\n",
       "      <td>0.06095</td>\n",
       "      <td>0.035920</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.1339</td>\n",
       "      <td>0.05945</td>\n",
       "      <td>...</td>\n",
       "      <td>13.330</td>\n",
       "      <td>25.48</td>\n",
       "      <td>86.16</td>\n",
       "      <td>546.7</td>\n",
       "      <td>0.12710</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.104600</td>\n",
       "      <td>0.069680</td>\n",
       "      <td>0.1712</td>\n",
       "      <td>0.07343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>12.220</td>\n",
       "      <td>20.04</td>\n",
       "      <td>79.47</td>\n",
       "      <td>453.1</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.11520</td>\n",
       "      <td>0.081750</td>\n",
       "      <td>0.021660</td>\n",
       "      <td>0.2124</td>\n",
       "      <td>0.06894</td>\n",
       "      <td>...</td>\n",
       "      <td>13.160</td>\n",
       "      <td>24.17</td>\n",
       "      <td>85.13</td>\n",
       "      <td>515.3</td>\n",
       "      <td>0.14020</td>\n",
       "      <td>0.23150</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>0.080880</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>0.08839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>13.490</td>\n",
       "      <td>22.30</td>\n",
       "      <td>86.91</td>\n",
       "      <td>561.0</td>\n",
       "      <td>0.08752</td>\n",
       "      <td>0.07698</td>\n",
       "      <td>0.047510</td>\n",
       "      <td>0.033840</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05718</td>\n",
       "      <td>...</td>\n",
       "      <td>15.150</td>\n",
       "      <td>31.82</td>\n",
       "      <td>99.00</td>\n",
       "      <td>698.8</td>\n",
       "      <td>0.11620</td>\n",
       "      <td>0.17110</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>0.128200</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.06917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>13.440</td>\n",
       "      <td>21.58</td>\n",
       "      <td>86.18</td>\n",
       "      <td>563.0</td>\n",
       "      <td>0.08162</td>\n",
       "      <td>0.06031</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.1784</td>\n",
       "      <td>0.05587</td>\n",
       "      <td>...</td>\n",
       "      <td>15.930</td>\n",
       "      <td>30.25</td>\n",
       "      <td>102.50</td>\n",
       "      <td>787.9</td>\n",
       "      <td>0.10940</td>\n",
       "      <td>0.20430</td>\n",
       "      <td>0.208500</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.2994</td>\n",
       "      <td>0.07146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>13.900</td>\n",
       "      <td>16.62</td>\n",
       "      <td>88.97</td>\n",
       "      <td>599.4</td>\n",
       "      <td>0.06828</td>\n",
       "      <td>0.05319</td>\n",
       "      <td>0.022240</td>\n",
       "      <td>0.013390</td>\n",
       "      <td>0.1813</td>\n",
       "      <td>0.05536</td>\n",
       "      <td>...</td>\n",
       "      <td>15.140</td>\n",
       "      <td>21.80</td>\n",
       "      <td>101.20</td>\n",
       "      <td>718.9</td>\n",
       "      <td>0.09384</td>\n",
       "      <td>0.20060</td>\n",
       "      <td>0.138400</td>\n",
       "      <td>0.062220</td>\n",
       "      <td>0.2679</td>\n",
       "      <td>0.07698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>15.470</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.52490</td>\n",
       "      <td>0.535500</td>\n",
       "      <td>0.174100</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>9.777</td>\n",
       "      <td>16.99</td>\n",
       "      <td>62.50</td>\n",
       "      <td>290.2</td>\n",
       "      <td>0.10370</td>\n",
       "      <td>0.08404</td>\n",
       "      <td>0.043340</td>\n",
       "      <td>0.017780</td>\n",
       "      <td>0.1584</td>\n",
       "      <td>0.07065</td>\n",
       "      <td>...</td>\n",
       "      <td>11.050</td>\n",
       "      <td>21.47</td>\n",
       "      <td>71.68</td>\n",
       "      <td>367.0</td>\n",
       "      <td>0.14670</td>\n",
       "      <td>0.17650</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.053340</td>\n",
       "      <td>0.2533</td>\n",
       "      <td>0.08468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>12.180</td>\n",
       "      <td>14.08</td>\n",
       "      <td>77.25</td>\n",
       "      <td>461.4</td>\n",
       "      <td>0.07734</td>\n",
       "      <td>0.03212</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.05649</td>\n",
       "      <td>...</td>\n",
       "      <td>12.850</td>\n",
       "      <td>16.47</td>\n",
       "      <td>81.60</td>\n",
       "      <td>513.1</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.05332</td>\n",
       "      <td>0.041160</td>\n",
       "      <td>0.018520</td>\n",
       "      <td>0.2293</td>\n",
       "      <td>0.06037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>12.770</td>\n",
       "      <td>29.43</td>\n",
       "      <td>81.35</td>\n",
       "      <td>507.9</td>\n",
       "      <td>0.08276</td>\n",
       "      <td>0.04234</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.05637</td>\n",
       "      <td>...</td>\n",
       "      <td>13.870</td>\n",
       "      <td>36.00</td>\n",
       "      <td>88.10</td>\n",
       "      <td>594.7</td>\n",
       "      <td>0.12340</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.086530</td>\n",
       "      <td>0.064980</td>\n",
       "      <td>0.2407</td>\n",
       "      <td>0.06484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>21.610</td>\n",
       "      <td>22.28</td>\n",
       "      <td>144.40</td>\n",
       "      <td>1407.0</td>\n",
       "      <td>0.11670</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.156200</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.06606</td>\n",
       "      <td>...</td>\n",
       "      <td>26.230</td>\n",
       "      <td>28.74</td>\n",
       "      <td>172.00</td>\n",
       "      <td>2081.0</td>\n",
       "      <td>0.15020</td>\n",
       "      <td>0.57170</td>\n",
       "      <td>0.705300</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.3828</td>\n",
       "      <td>0.10070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>16.070</td>\n",
       "      <td>19.65</td>\n",
       "      <td>104.10</td>\n",
       "      <td>817.7</td>\n",
       "      <td>0.09168</td>\n",
       "      <td>0.08424</td>\n",
       "      <td>0.097690</td>\n",
       "      <td>0.066380</td>\n",
       "      <td>0.1798</td>\n",
       "      <td>0.05391</td>\n",
       "      <td>...</td>\n",
       "      <td>19.770</td>\n",
       "      <td>24.56</td>\n",
       "      <td>128.80</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>0.15000</td>\n",
       "      <td>0.20450</td>\n",
       "      <td>0.282900</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.06387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>18.770</td>\n",
       "      <td>21.43</td>\n",
       "      <td>122.90</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>0.09116</td>\n",
       "      <td>0.14020</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.1953</td>\n",
       "      <td>0.06083</td>\n",
       "      <td>...</td>\n",
       "      <td>24.540</td>\n",
       "      <td>34.37</td>\n",
       "      <td>161.10</td>\n",
       "      <td>1873.0</td>\n",
       "      <td>0.14980</td>\n",
       "      <td>0.48270</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.204800</td>\n",
       "      <td>0.3679</td>\n",
       "      <td>0.09870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>13.710</td>\n",
       "      <td>18.68</td>\n",
       "      <td>88.73</td>\n",
       "      <td>571.0</td>\n",
       "      <td>0.09916</td>\n",
       "      <td>0.10700</td>\n",
       "      <td>0.053850</td>\n",
       "      <td>0.037830</td>\n",
       "      <td>0.1714</td>\n",
       "      <td>0.06843</td>\n",
       "      <td>...</td>\n",
       "      <td>15.110</td>\n",
       "      <td>25.63</td>\n",
       "      <td>99.43</td>\n",
       "      <td>701.9</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.25660</td>\n",
       "      <td>0.193500</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.2849</td>\n",
       "      <td>0.09031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "178       13.010         22.22           82.01      526.4          0.06251   \n",
       "421       14.690         13.98           98.22      656.1          0.10310   \n",
       "57        14.710         21.59           95.55      656.9          0.11370   \n",
       "514       15.050         19.07           97.26      701.9          0.09215   \n",
       "548        9.683         19.34           61.05      285.7          0.08491   \n",
       "456       11.630         29.29           74.87      415.1          0.09357   \n",
       "511       14.810         14.70           94.66      680.7          0.08472   \n",
       "69        12.780         16.49           81.37      502.5          0.09831   \n",
       "410       11.360         17.57           72.49      399.8          0.08858   \n",
       "382       12.050         22.72           78.75      447.8          0.06935   \n",
       "41        10.950         21.35           71.90      371.1          0.12270   \n",
       "70        18.940         21.31          123.60     1130.0          0.09009   \n",
       "183       11.410         14.92           73.53      402.0          0.09059   \n",
       "333       11.250         14.78           71.38      390.0          0.08306   \n",
       "324       12.200         15.21           78.01      457.9          0.08673   \n",
       "392       15.490         19.97          102.40      744.7          0.11600   \n",
       "427       10.800         21.98           68.79      359.9          0.08801   \n",
       "43        13.280         20.28           87.32      545.2          0.10410   \n",
       "36        14.250         21.72           93.63      633.0          0.09823   \n",
       "209       15.270         12.91           98.17      725.5          0.08182   \n",
       "330       16.030         15.51          105.80      793.2          0.09491   \n",
       "344       11.710         15.45           75.03      420.3          0.11500   \n",
       "555       10.290         27.61           65.67      321.4          0.09030   \n",
       "170       12.320         12.39           78.85      464.1          0.10280   \n",
       "375       16.170         16.07          106.30      788.5          0.09880   \n",
       "143       12.900         15.92           83.74      512.2          0.08677   \n",
       "360       12.540         18.07           79.42      491.9          0.07436   \n",
       "449       21.100         20.52          138.10     1384.0          0.09684   \n",
       "527       12.340         12.27           78.94      468.5          0.09003   \n",
       "227       15.000         15.51           97.45      684.5          0.08371   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "440       10.970         17.20           71.73      371.5          0.08915   \n",
       "436       12.870         19.54           82.67      509.2          0.09136   \n",
       "519       12.750         16.70           82.51      493.8          0.11250   \n",
       "284       12.890         15.70           84.08      516.6          0.07818   \n",
       "99        14.420         19.77           94.48      642.5          0.09752   \n",
       "264       17.190         22.07          111.60      928.3          0.09726   \n",
       "239       17.460         39.28          113.40      920.6          0.09812   \n",
       "457       13.210         25.25           84.10      537.9          0.08791   \n",
       "503       23.090         19.83          152.10     1682.0          0.09342   \n",
       "105       13.110         15.56           87.21      530.2          0.13980   \n",
       "343       19.680         21.68          129.90     1194.0          0.09797   \n",
       "76        13.530         10.94           87.91      559.2          0.12910   \n",
       "275       11.890         17.36           76.20      435.6          0.12250   \n",
       "249       11.520         14.93           73.87      406.3          0.10130   \n",
       "418       12.700         12.17           80.88      495.0          0.08785   \n",
       "59         8.618         11.79           54.34      224.5          0.09752   \n",
       "371       15.190         13.21           97.65      711.8          0.07963   \n",
       "136       11.710         16.67           74.72      423.6          0.10510   \n",
       "506       12.220         20.04           79.47      453.1          0.10960   \n",
       "49        13.490         22.30           86.91      561.0          0.08752   \n",
       "40        13.440         21.58           86.18      563.0          0.08162   \n",
       "477       13.900         16.62           88.97      599.4          0.06828   \n",
       "5         12.450         15.70           82.57      477.1          0.12780   \n",
       "110        9.777         16.99           62.50      290.2          0.10370   \n",
       "316       12.180         14.08           77.25      461.4          0.07734   \n",
       "552       12.770         29.43           81.35      507.9          0.08276   \n",
       "393       21.610         22.28          144.40     1407.0          0.11670   \n",
       "75        16.070         19.65          104.10      817.7          0.09168   \n",
       "337       18.770         21.43          122.90     1092.0          0.09116   \n",
       "523       13.710         18.68           88.73      571.0          0.09916   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "178           0.01938        0.001595             0.001852         0.1395   \n",
       "421           0.18360        0.145000             0.063000         0.2086   \n",
       "57            0.13650        0.129300             0.081230         0.2027   \n",
       "514           0.08597        0.074860             0.043350         0.1561   \n",
       "548           0.05030        0.023370             0.009615         0.1580   \n",
       "456           0.08574        0.071600             0.020170         0.1799   \n",
       "511           0.05016        0.034160             0.025410         0.1659   \n",
       "69            0.05234        0.036530             0.028640         0.1590   \n",
       "410           0.05313        0.027830             0.021000         0.1601   \n",
       "382           0.10730        0.079430             0.029780         0.1203   \n",
       "41            0.12180        0.104400             0.056690         0.1895   \n",
       "70            0.10290        0.108000             0.079510         0.1582   \n",
       "183           0.08155        0.061810             0.023610         0.1167   \n",
       "333           0.04458        0.000974             0.002941         0.1773   \n",
       "324           0.06545        0.019940             0.016920         0.1638   \n",
       "392           0.15620        0.189100             0.091130         0.1929   \n",
       "427           0.05743        0.036140             0.014040         0.2016   \n",
       "43            0.14360        0.098470             0.061580         0.1974   \n",
       "36            0.10980        0.131900             0.055980         0.1885   \n",
       "209           0.06230        0.058920             0.031570         0.1359   \n",
       "330           0.13710        0.120400             0.070410         0.1782   \n",
       "344           0.07281        0.040060             0.032500         0.2009   \n",
       "555           0.07658        0.059990             0.027380         0.1593   \n",
       "170           0.06981        0.039870             0.037000         0.1959   \n",
       "375           0.14380        0.066510             0.053970         0.1990   \n",
       "143           0.09509        0.048940             0.030880         0.1778   \n",
       "360           0.02650        0.001194             0.005449         0.1528   \n",
       "449           0.11750        0.157200             0.115500         0.1554   \n",
       "527           0.06307        0.029580             0.026470         0.1689   \n",
       "227           0.10960        0.065050             0.037800         0.1881   \n",
       "..                ...             ...                  ...            ...   \n",
       "440           0.11130        0.094570             0.036130         0.1489   \n",
       "436           0.07883        0.017970             0.020900         0.1861   \n",
       "519           0.11170        0.038800             0.029950         0.2120   \n",
       "284           0.09580        0.111500             0.033900         0.1432   \n",
       "99            0.11410        0.093880             0.058390         0.1879   \n",
       "264           0.08995        0.090610             0.065270         0.1867   \n",
       "239           0.12980        0.141700             0.088110         0.1809   \n",
       "457           0.05205        0.027720             0.020680         0.1619   \n",
       "503           0.12750        0.167600             0.100300         0.1505   \n",
       "105           0.17650        0.207100             0.096010         0.1925   \n",
       "343           0.13390        0.186300             0.110300         0.2082   \n",
       "76            0.10470        0.068770             0.065560         0.2403   \n",
       "275           0.07210        0.059290             0.074040         0.2015   \n",
       "249           0.07808        0.043280             0.029290         0.1883   \n",
       "418           0.05794        0.023600             0.024020         0.1583   \n",
       "59            0.05272        0.020610             0.007799         0.1683   \n",
       "371           0.06934        0.033930             0.026570         0.1721   \n",
       "136           0.06095        0.035920             0.026000         0.1339   \n",
       "506           0.11520        0.081750             0.021660         0.2124   \n",
       "49            0.07698        0.047510             0.033840         0.1809   \n",
       "40            0.06031        0.031100             0.020310         0.1784   \n",
       "477           0.05319        0.022240             0.013390         0.1813   \n",
       "5             0.17000        0.157800             0.080890         0.2087   \n",
       "110           0.08404        0.043340             0.017780         0.1584   \n",
       "316           0.03212        0.011230             0.005051         0.1673   \n",
       "552           0.04234        0.019970             0.014990         0.1539   \n",
       "393           0.20870        0.281000             0.156200         0.2162   \n",
       "75            0.08424        0.097690             0.066380         0.1798   \n",
       "337           0.14020        0.106000             0.060900         0.1953   \n",
       "523           0.10700        0.053850             0.037830         0.1714   \n",
       "\n",
       "     mean fractal dimension           ...             worst radius  \\\n",
       "178                 0.05234           ...                   14.000   \n",
       "421                 0.07406           ...                   16.460   \n",
       "57                  0.06758           ...                   17.870   \n",
       "514                 0.05915           ...                   17.580   \n",
       "548                 0.06235           ...                   10.930   \n",
       "456                 0.06166           ...                   13.120   \n",
       "511                 0.05348           ...                   15.610   \n",
       "69                  0.05653           ...                   13.460   \n",
       "410                 0.05913           ...                   13.050   \n",
       "382                 0.06659           ...                   12.570   \n",
       "41                  0.06870           ...                   12.840   \n",
       "70                  0.05461           ...                   24.860   \n",
       "183                 0.06217           ...                   12.370   \n",
       "333                 0.06081           ...                   12.760   \n",
       "324                 0.06129           ...                   13.750   \n",
       "392                 0.06744           ...                   21.200   \n",
       "427                 0.05977           ...                   12.760   \n",
       "43                  0.06782           ...                   17.380   \n",
       "36                  0.06125           ...                   15.890   \n",
       "209                 0.05526           ...                   17.380   \n",
       "330                 0.05976           ...                   18.760   \n",
       "344                 0.06506           ...                   13.060   \n",
       "555                 0.06127           ...                   10.840   \n",
       "170                 0.05955           ...                   13.500   \n",
       "375                 0.06572           ...                   16.970   \n",
       "143                 0.06235           ...                   14.480   \n",
       "360                 0.05185           ...                   13.720   \n",
       "449                 0.05661           ...                   25.680   \n",
       "527                 0.05808           ...                   13.610   \n",
       "227                 0.05907           ...                   16.410   \n",
       "..                      ...           ...                      ...   \n",
       "440                 0.06640           ...                   12.360   \n",
       "436                 0.06347           ...                   14.450   \n",
       "519                 0.06623           ...                   14.450   \n",
       "284                 0.05935           ...                   13.900   \n",
       "99                  0.06390           ...                   16.330   \n",
       "264                 0.05580           ...                   21.580   \n",
       "239                 0.05966           ...                   22.510   \n",
       "457                 0.05584           ...                   14.350   \n",
       "503                 0.05484           ...                   30.790   \n",
       "105                 0.07692           ...                   16.310   \n",
       "343                 0.05715           ...                   22.750   \n",
       "76                  0.06641           ...                   14.080   \n",
       "275                 0.05875           ...                   12.400   \n",
       "249                 0.06168           ...                   12.650   \n",
       "418                 0.06275           ...                   13.650   \n",
       "59                  0.07187           ...                    9.507   \n",
       "371                 0.05544           ...                   16.200   \n",
       "136                 0.05945           ...                   13.330   \n",
       "506                 0.06894           ...                   13.160   \n",
       "49                  0.05718           ...                   15.150   \n",
       "40                  0.05587           ...                   15.930   \n",
       "477                 0.05536           ...                   15.140   \n",
       "5                   0.07613           ...                   15.470   \n",
       "110                 0.07065           ...                   11.050   \n",
       "316                 0.05649           ...                   12.850   \n",
       "552                 0.05637           ...                   13.870   \n",
       "393                 0.06606           ...                   26.230   \n",
       "75                  0.05391           ...                   19.770   \n",
       "337                 0.06083           ...                   24.540   \n",
       "523                 0.06843           ...                   15.110   \n",
       "\n",
       "     worst texture  worst perimeter  worst area  worst smoothness  \\\n",
       "178          29.02            88.18       608.8           0.08125   \n",
       "421          18.34           114.10       809.2           0.13120   \n",
       "57           30.70           115.70       985.5           0.13680   \n",
       "514          28.06           113.80       967.0           0.12460   \n",
       "548          25.59            69.10       364.2           0.11990   \n",
       "456          38.81            86.04       527.8           0.14060   \n",
       "511          17.58           101.70       760.2           0.11390   \n",
       "69           19.76            85.67       554.9           0.12960   \n",
       "410          36.32            85.07       521.3           0.14530   \n",
       "382          28.71            87.36       488.4           0.08799   \n",
       "41           35.34            87.22       514.0           0.19090   \n",
       "70           26.58           165.90      1866.0           0.11930   \n",
       "183          17.70            79.12       467.2           0.11210   \n",
       "333          22.06            82.08       492.7           0.11660   \n",
       "324          21.38            91.11       583.1           0.12560   \n",
       "392          29.41           142.10      1359.0           0.16810   \n",
       "427          32.04            83.69       489.5           0.13030   \n",
       "43           28.00           113.10       907.2           0.15300   \n",
       "36           30.36           116.20       799.6           0.14460   \n",
       "209          15.92           113.70       932.7           0.12220   \n",
       "330          21.98           124.30      1070.0           0.14350   \n",
       "344          18.16            84.16       516.4           0.14600   \n",
       "555          34.91            69.57       357.6           0.13840   \n",
       "170          15.64            86.97       549.1           0.13850   \n",
       "375          19.14           113.10       861.5           0.12350   \n",
       "143          21.82            97.17       643.8           0.13120   \n",
       "360          20.98            86.82       585.7           0.09293   \n",
       "449          32.07           168.20      2022.0           0.13680   \n",
       "527          19.27            87.22       564.9           0.12920   \n",
       "227          19.31           114.20       808.2           0.11360   \n",
       "..             ...              ...         ...               ...   \n",
       "440          26.87            90.14       476.4           0.13910   \n",
       "436          24.38            95.14       626.9           0.12140   \n",
       "519          21.74            93.63       624.1           0.14750   \n",
       "284          19.69            92.12       595.6           0.09926   \n",
       "99           30.86           109.50       826.4           0.14310   \n",
       "264          29.33           140.50      1436.0           0.15580   \n",
       "239          44.87           141.20      1408.0           0.13650   \n",
       "457          34.23            91.29       632.9           0.12890   \n",
       "503          23.87           211.50      2782.0           0.11990   \n",
       "105          22.40           106.40       827.2           0.18620   \n",
       "343          34.66           157.60      1540.0           0.12180   \n",
       "76           12.49            91.36       605.5           0.14510   \n",
       "275          18.99            79.46       472.4           0.13590   \n",
       "249          21.19            80.88       491.8           0.13890   \n",
       "418          16.92            88.12       566.9           0.13140   \n",
       "59           15.40            59.90       274.9           0.17330   \n",
       "371          15.73           104.50       819.1           0.11260   \n",
       "136          25.48            86.16       546.7           0.12710   \n",
       "506          24.17            85.13       515.3           0.14020   \n",
       "49           31.82            99.00       698.8           0.11620   \n",
       "40           30.25           102.50       787.9           0.10940   \n",
       "477          21.80           101.20       718.9           0.09384   \n",
       "5            23.75           103.40       741.6           0.17910   \n",
       "110          21.47            71.68       367.0           0.14670   \n",
       "316          16.47            81.60       513.1           0.10010   \n",
       "552          36.00            88.10       594.7           0.12340   \n",
       "393          28.74           172.00      2081.0           0.15020   \n",
       "75           24.56           128.80      1223.0           0.15000   \n",
       "337          34.37           161.10      1873.0           0.14980   \n",
       "523          25.63            99.43       701.9           0.14250   \n",
       "\n",
       "     worst compactness  worst concavity  worst concave points  worst symmetry  \\\n",
       "178            0.03432         0.007977              0.009259          0.2295   \n",
       "421            0.36350         0.321900              0.110800          0.2827   \n",
       "57             0.42900         0.358700              0.183400          0.3698   \n",
       "514            0.21010         0.286600              0.112000          0.2282   \n",
       "548            0.09546         0.093500              0.038460          0.2552   \n",
       "456            0.20310         0.292300              0.068350          0.2884   \n",
       "511            0.10110         0.110100              0.079550          0.2334   \n",
       "69             0.07061         0.103900              0.058820          0.2383   \n",
       "410            0.16220         0.181100              0.086980          0.2973   \n",
       "382            0.32140         0.291200              0.109200          0.2191   \n",
       "41             0.26980         0.402300              0.142400          0.2964   \n",
       "70             0.23360         0.268700              0.178900          0.2551   \n",
       "183            0.16100         0.164800              0.062960          0.1811   \n",
       "333            0.09794         0.005518              0.016670          0.2815   \n",
       "324            0.19280         0.116700              0.055560          0.2661   \n",
       "392            0.39130         0.555300              0.212100          0.3187   \n",
       "427            0.16960         0.192700              0.074850          0.2965   \n",
       "43             0.37240         0.366400              0.149200          0.3739   \n",
       "36             0.42380         0.518600              0.144700          0.3591   \n",
       "209            0.21860         0.296200              0.103500          0.2320   \n",
       "330            0.44780         0.495600              0.198100          0.3019   \n",
       "344            0.11150         0.108700              0.078640          0.2765   \n",
       "555            0.17100         0.200000              0.091270          0.2226   \n",
       "170            0.12660         0.124200              0.093910          0.2827   \n",
       "375            0.25500         0.211400              0.125100          0.3153   \n",
       "143            0.25480         0.209000              0.101200          0.3549   \n",
       "360            0.04327         0.003581              0.016350          0.2233   \n",
       "449            0.31010         0.439900              0.228000          0.2268   \n",
       "527            0.20740         0.179100              0.107000          0.3110   \n",
       "227            0.36270         0.340200              0.137900          0.2954   \n",
       "..                 ...              ...                   ...             ...   \n",
       "440            0.40820         0.477900              0.155500          0.2540   \n",
       "436            0.16520         0.071270              0.063840          0.3313   \n",
       "519            0.19790         0.142300              0.080450          0.3071   \n",
       "284            0.23170         0.334400              0.101700          0.1999   \n",
       "99             0.30260         0.319400              0.156500          0.2718   \n",
       "264            0.25670         0.388900              0.198400          0.3216   \n",
       "239            0.37350         0.324100              0.206600          0.2853   \n",
       "457            0.10630         0.139000              0.060050          0.2444   \n",
       "503            0.36250         0.379400              0.226400          0.2908   \n",
       "105            0.40990         0.637600              0.198600          0.3147   \n",
       "343            0.34580         0.473400              0.225500          0.4045   \n",
       "76             0.13790         0.085390              0.074070          0.2710   \n",
       "275            0.08368         0.071530              0.089460          0.2220   \n",
       "249            0.15820         0.180400              0.096080          0.2664   \n",
       "418            0.16070         0.093850              0.082240          0.2775   \n",
       "59             0.12390         0.116800              0.044190          0.3220   \n",
       "371            0.17370         0.136200              0.081780          0.2487   \n",
       "136            0.10280         0.104600              0.069680          0.1712   \n",
       "506            0.23150         0.353500              0.080880          0.2709   \n",
       "49             0.17110         0.228200              0.128200          0.2871   \n",
       "40             0.20430         0.208500              0.111200          0.2994   \n",
       "477            0.20060         0.138400              0.062220          0.2679   \n",
       "5              0.52490         0.535500              0.174100          0.3985   \n",
       "110            0.17650         0.130000              0.053340          0.2533   \n",
       "316            0.05332         0.041160              0.018520          0.2293   \n",
       "552            0.10640         0.086530              0.064980          0.2407   \n",
       "393            0.57170         0.705300              0.242200          0.3828   \n",
       "75             0.20450         0.282900              0.152000          0.2650   \n",
       "337            0.48270         0.463400              0.204800          0.3679   \n",
       "523            0.25660         0.193500              0.128400          0.2849   \n",
       "\n",
       "     worst fractal dimension  \n",
       "178                  0.05843  \n",
       "421                  0.09208  \n",
       "57                   0.10940  \n",
       "514                  0.06954  \n",
       "548                  0.07920  \n",
       "456                  0.07220  \n",
       "511                  0.06142  \n",
       "69                   0.06410  \n",
       "410                  0.07745  \n",
       "382                  0.09349  \n",
       "41                   0.09606  \n",
       "70                   0.06589  \n",
       "183                  0.07427  \n",
       "333                  0.07418  \n",
       "324                  0.07961  \n",
       "392                  0.10190  \n",
       "427                  0.07662  \n",
       "43                   0.10270  \n",
       "36                   0.10140  \n",
       "209                  0.07474  \n",
       "330                  0.09124  \n",
       "344                  0.07806  \n",
       "555                  0.08283  \n",
       "170                  0.06771  \n",
       "375                  0.08960  \n",
       "143                  0.08118  \n",
       "360                  0.05521  \n",
       "449                  0.07425  \n",
       "527                  0.07592  \n",
       "227                  0.08362  \n",
       "..                       ...  \n",
       "440                  0.09532  \n",
       "436                  0.07735  \n",
       "519                  0.08557  \n",
       "284                  0.07127  \n",
       "99                   0.09353  \n",
       "264                  0.07570  \n",
       "239                  0.08496  \n",
       "457                  0.06788  \n",
       "503                  0.07277  \n",
       "105                  0.14050  \n",
       "343                  0.07918  \n",
       "76                   0.07191  \n",
       "275                  0.06033  \n",
       "249                  0.07809  \n",
       "418                  0.09464  \n",
       "59                   0.09026  \n",
       "371                  0.06766  \n",
       "136                  0.07343  \n",
       "506                  0.08839  \n",
       "49                   0.06917  \n",
       "40                   0.07146  \n",
       "477                  0.07698  \n",
       "5                    0.12440  \n",
       "110                  0.08468  \n",
       "316                  0.06037  \n",
       "552                  0.06484  \n",
       "393                  0.10070  \n",
       "75                   0.06387  \n",
       "337                  0.09870  \n",
       "523                  0.09031  \n",
       "\n",
       "[398 rows x 30 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando o Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVM = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/Python/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SVM.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previsões e avaliações\n",
    "\n",
    "Agora vamos prever o uso do modelo treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_SVM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        66\n",
      "           1       0.61      1.00      0.76       105\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       171\n",
      "   macro avg       0.31      0.50      0.38       171\n",
      "weighted avg       0.38      0.61      0.47       171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/Python/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/felipe/Python/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/felipe/Python/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  66]\n",
      " [  0 105]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notemos que o desempenho foi horroroso. Note que estamos classificando tudo em uma única classe! Isso significa que nosso modelo precisa ter parâmetros ajustados (também pode ajudar a normalizar os dados).\n",
    "\n",
    "Para podermos ajustar os parâmetros do Support Vector Classifier podemos usar um método do Scikit Learning conhecido como GridSearch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch\n",
    "\n",
    "Encontrar os parâmetros certos (como o que o C ou os valores de gama para usar) é uma tarefa complicada! Mas, felizmente, podemos ser um pouco preguiçosos e apenas tentar um monte de combinações e ver o que funciona melhor! Essa idéia de criar uma \"grade\" de parâmetros e apenas experimentar todas as combinações possíveis é chamada Gridsearch, esse método é comum o suficiente para que o Scikit-learn tenha essa funcionalidade incorporada no GridSearchCV!\n",
    "\n",
    "O GridSearchCV usa um dicionário que descreve os parâmetros que devem ser testados e um modelo para treinar. A grade de parâmetros é definida como um dicionário, onde as chaves são os parâmetros e os valores são as configurações a serem testadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O GridSearch precisa como input um dicionário em que nós especificamos os valores dos parâmetros que queremos que sejam testados para depois ele ver qual foi a melhor combinação deles. Nesse caso daqui vamos variar os parâmetros 'C' e 'gamma'. E vamos testar no kernel apenas a opção 'rbf'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C' : [0.1, 1, 10, 100, 1000], 'gamma' : [1, 0.1, 0.01, 0.001, 0.0001], 'kernel' : ['rbf']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma das grandes coisas do GridSearchCV é que é um meta-estimador. Ele pega um estimador como SVC e cria um novo estimador, que se comporta exatamente da mesma forma - neste caso, como um classificador. Você deve adicionar refit=True e escolher detalhadamente para  número desejado, maior o número, mais detalhado. \n",
    "\n",
    "Você deve adicionar refit=True e escolher  um número para o parâmetro verbose. Quanto maior o número, mais detalhamento teremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid = GridSearchCV(SVC(),param_grid, refit=True, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que o fit faz é um pouco mais complicado do que o habitual. Primeiro, ele executa o mesmo loop com validação cruzada, para encontrar a melhor combinação de parâmetros. Uma vez que tenha a melhor combinação, ele corre novamente em todos os dados para fitá-los (sem validação cruzada), para construir um único modelo novo usando a melhor configuração de parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/Python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV]  C=0.1, gamma=1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV]  C=0.1, gamma=1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV]  C=0.1, gamma=1, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=0.1, gamma=0.1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=0.1, gamma=0.1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=0.1, gamma=0.1, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=0.1, gamma=0.01, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=0.1, gamma=0.01, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=0.1, gamma=0.01, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=0.1, gamma=0.001, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=0.1, gamma=0.001, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=0.1, gamma=0.001, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=0.1, gamma=0.0001, kernel=rbf, score=0.9022556390977443, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=0.1, gamma=0.0001, kernel=rbf, score=0.9624060150375939, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=0.1, gamma=0.0001, kernel=rbf, score=0.9166666666666666, total=   0.0s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV]  C=1, gamma=1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV]  C=1, gamma=1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV]  C=1, gamma=1, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV]  C=1, gamma=0.1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV]  C=1, gamma=0.1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV]  C=1, gamma=0.1, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV]  C=1, gamma=0.01, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV]  C=1, gamma=0.01, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV]  C=1, gamma=0.01, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV]  C=1, gamma=0.001, kernel=rbf, score=0.9022556390977443, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV]  C=1, gamma=0.001, kernel=rbf, score=0.9398496240601504, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV]  C=1, gamma=0.001, kernel=rbf, score=0.9545454545454546, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=rbf, score=0.9398496240601504, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=rbf, score=0.9699248120300752, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=rbf, score=0.946969696969697, total=   0.0s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV]  C=10, gamma=1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV]  C=10, gamma=1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV]  C=10, gamma=1, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV]  C=10, gamma=0.1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV]  C=10, gamma=0.1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV]  C=10, gamma=0.1, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV]  C=10, gamma=0.01, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV]  C=10, gamma=0.01, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV]  C=10, gamma=0.01, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV]  C=10, gamma=0.001, kernel=rbf, score=0.8947368421052632, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV]  C=10, gamma=0.001, kernel=rbf, score=0.9323308270676691, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV]  C=10, gamma=0.001, kernel=rbf, score=0.9166666666666666, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV]  C=10, gamma=0.0001, kernel=rbf, score=0.9323308270676691, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV]  C=10, gamma=0.0001, kernel=rbf, score=0.9699248120300752, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV]  C=10, gamma=0.0001, kernel=rbf, score=0.9621212121212122, total=   0.0s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV]  C=100, gamma=1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV]  C=100, gamma=1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV]  C=100, gamma=1, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=100, gamma=0.1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=100, gamma=0.1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=100, gamma=0.1, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=100, gamma=0.01, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=100, gamma=0.01, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=100, gamma=0.01, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=100, gamma=0.001, kernel=rbf, score=0.8947368421052632, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=100, gamma=0.001, kernel=rbf, score=0.9323308270676691, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, gamma=0.001, kernel=rbf, score=0.9166666666666666, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=100, gamma=0.0001, kernel=rbf, score=0.9172932330827067, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=100, gamma=0.0001, kernel=rbf, score=0.9774436090225563, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=100, gamma=0.0001, kernel=rbf, score=0.9393939393939394, total=   0.0s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV]  C=1000, gamma=1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV]  C=1000, gamma=1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV]  C=1000, gamma=1, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV]  C=1000, gamma=0.1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV]  C=1000, gamma=0.1, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV]  C=1000, gamma=0.1, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV]  C=1000, gamma=0.01, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV]  C=1000, gamma=0.01, kernel=rbf, score=0.631578947368421, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV]  C=1000, gamma=0.01, kernel=rbf, score=0.6363636363636364, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV]  C=1000, gamma=0.001, kernel=rbf, score=0.8947368421052632, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV]  C=1000, gamma=0.001, kernel=rbf, score=0.9323308270676691, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV]  C=1000, gamma=0.001, kernel=rbf, score=0.9166666666666666, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.9097744360902256, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.9699248120300752, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.9318181818181818, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora podemos verificar qual foi a melhor combinação de parâmetros que o GridSearch encontrou e também o melhor estimador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid_Predictions = Grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93        66\n",
      "           1       0.94      0.97      0.96       105\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       171\n",
      "   macro avg       0.95      0.94      0.94       171\n",
      "weighted avg       0.95      0.95      0.95       171\n",
      "\n",
      "\n",
      "\n",
      "[[ 60   6]\n",
      " [  3 102]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Grid_Predictions))\n",
    "print('\\n')\n",
    "print(confusion_matrix(Y_test, Grid_Predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notemos que com o uso do GridSearch o nosso modelo de Support Vector Machine ficou muito melhor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
